{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3842ba7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports:\n",
    "from ncps.wirings import AutoNCP\n",
    "from ncps.torch import CfC\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from numpy import genfromtxt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import matplotlib as plt\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1a6fb66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4957201, 12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\will\\AppData\\Local\\Temp\\ipykernel_8872\\670503757.py:27: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:257.)\n",
      "  testingIn = torch.Tensor(newsetIn)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nCreate Dataloader from dataset. Shuffle off.\\n10 BSMs per batch, each BSM with 7 features. Dataset is organized by sender, and a multiple of 100. Dataset is 'reflectedly' padded, ie. it repeates the end entries until a multiple of 100.\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Formatting dataset:\n",
    "#Time sequences are 10 timepoints (Messages) with 7 features per message.\n",
    "#Organized by car.\n",
    "\n",
    "#Current Simulation File\n",
    "dataFile = 'Data/CfCMultiExtension/DoS_0709.csv'\n",
    "\n",
    "dataSet = genfromtxt(dataFile, delimiter=',')\n",
    "# Ceate dataloader and fill with (BSM, attk#). Expanding to add 0th dimension for batches.\n",
    "# Batch size of 10 in accordance to our time sequences.\n",
    "# No shuffle to keep batches on same vehicle.\n",
    "# Num_workers is set to = num CPU cores\n",
    "dataSet[0:-1,:] = dataSet[1:,:]\n",
    "print(dataSet.shape)\n",
    "len = dataSet.shape[0]\n",
    "trainPerc = 80\n",
    "trainDataIn = torch.Tensor(dataSet[:int(len*(trainPerc/100)),1:10])\n",
    "trainDataOut = torch.Tensor(np.int_(dataSet[:int(len*(trainPerc/100)),11])).long()\n",
    "testDataIn = torch.Tensor(dataSet[int(len*(trainPerc/100)):,1:10])\n",
    "testDataOut = torch.Tensor(np.int_(dataSet[int(len*(trainPerc/100)):,11])).long()\n",
    "newsetIn = []\n",
    "newsetOut = []\n",
    "for index in range(0,int(len * (trainPerc/100))):\n",
    "    if not (int(index/10) % 100):\n",
    "        newsetIn.append(dataSet[index,1:10])\n",
    "        newsetOut.append((dataSet[index, 11]))\n",
    "testingIn = torch.Tensor(newsetIn)\n",
    "testingOut = torch.Tensor(newsetOut).long()\n",
    "dataLoaderTrain = data.DataLoader(data.TensorDataset(trainDataIn, trainDataOut), batch_size=10, shuffle=False, num_workers=16)\n",
    "dataLoaderTest = data.DataLoader(data.TensorDataset(testDataIn, testDataOut), batch_size=10, shuffle=False, num_workers=16)\n",
    "testingDataLoader = data.DataLoader(data.TensorDataset(testingIn, testingOut), batch_size=10, shuffle = False, num_workers=16)\n",
    "\n",
    "'''\n",
    "Create Dataloader from dataset. Shuffle off.\n",
    "10 BSMs per batch, each BSM with 7 features. Dataset is organized by sender, and a multiple of 100. Dataset is 'reflectedly' padded, ie. it repeates the end entries until a multiple of 100.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "55f9ff02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3965760, 9])\n",
      "torch.Size([3965760])\n",
      "torch.Size([39660])\n",
      "torch.Size([39660, 9])\n",
      "tensor([1.9347e+04, 1.9611e+04, 3.1161e+04, 6.6291e+01, 2.4601e+02, 3.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00])\n",
      "tensor(0)\n",
      "tensor([1.9347e+04, 1.9611e+04, 3.1161e+04, 6.6291e+01, 2.4601e+02, 4.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00])\n",
      "tensor(0)\n",
      "[0.00000000e+00 4.50000000e+01 9.00000000e+00 2.52126029e+04\n",
      " 7.93602911e+00 9.57097011e+01 1.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      "(4957201, 12)\n"
     ]
    }
   ],
   "source": [
    "print(trainDataIn.shape)\n",
    "print(trainDataOut.shape)\n",
    "print(testingOut.shape)\n",
    "print(testingIn.shape)\n",
    "print(trainDataIn[-1])\n",
    "print(trainDataOut[1])\n",
    "print(testDataIn[15])\n",
    "print(testingOut[0])\n",
    "print(dataSet[0])\n",
    "print(dataSet.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f4ff8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Model\n",
    "#Create Learning function\n",
    "#Create Testing function\n",
    "#Create tests\n",
    "\n",
    "\n",
    "class CfCLearner(pl.LightningModule):\n",
    "    def __init__(self, model, lr):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.lossFunc = nn.CrossEntropyLoss()\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, target = batch\n",
    "        output, _ = self.model.forward(inputs)\n",
    "        loss = self.lossFunc(output, target)\n",
    "        self.log(\"trainLoss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs, target = batch\n",
    "        output, _ = self.model.forward(inputs)\n",
    "        print(f\"output: {output.shape}\")\n",
    "        print(f\"target: {target.shape}\")\n",
    "        loss = self.lossFunc(output, target)\n",
    "        self.log(\"valLoss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self.validation_step(batch, batch_idx)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.model.parameters(), lr = self.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28b52804",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Modena(nn.Module): \n",
    "    # CfC with feed-forward layer to classify at end.\n",
    "    def __init__(self, inputSize, unitNum, motorNum, outputDim, batchFirst = True):\n",
    "        super().__init__()\n",
    "        # Create NCP wiring for CfC\n",
    "        wiring = AutoNCP(unitNum, motorNum)\n",
    "        # Create CfC model with inputs and wiring\n",
    "        self.cfc = CfC(inputSize, wiring, batch_first=batchFirst)\n",
    "        # Create feed-forward layer\n",
    "        self.fF = nn.Linear(motorNum, outputDim)\n",
    "    \n",
    "    def forward(self, batch, hidden = None):\n",
    "        batch, hidden = self.cfc(batch, hidden) # Pass inputs through CfC\n",
    "        out = nn.functional.relu(self.fF(batch)) # pass through FeedForward Layer, then make 0 minimum\n",
    "        return out, hidden # Return the guess and the hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38b459e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, dataIn, dataOut):\n",
    "    with torch.no_grad():\n",
    "        outs = np.asarray(model(dataIn)[0])\n",
    "    outs = torch.from_numpy(outs)\n",
    "    _, res = torch.max(outs, 1)\n",
    "    countR = 0\n",
    "    numZero = 0\n",
    "    tot = outs.shape[0]\n",
    "    for i in range(0, tot):\n",
    "        if res[i] == dataOut[i]:\n",
    "            countR += 1\n",
    "        if dataOut[i] == 0:\n",
    "            numZero += 1\n",
    "    perc = (countR/tot) * 100\n",
    "    percZero = (numZero/tot) * 100\n",
    "    print(\"Model got \" + str(countR) + \"/\" + str(tot) + \" right. Accuracy of \" + str(perc) + \"%\")\n",
    "    print(str(percZero) + \"% Zeroes.\")\n",
    "    return countR, tot, perc, percZero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f62e03e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "#20 units, 8 motor neuron (output)\n",
    "\n",
    "#input size = 9, 20 units, 8 motor neurons, and 20 possible outputs.\n",
    "model = Modena(9, 20, 8, 20)\n",
    "learner = CfCLearner(model, lr=0.001) #Tune units, lr\n",
    "trainer = pl.Trainer(\n",
    "    logger = CSVLogger('log'), # Set ouput destination of logs, logging accuracy every 50 steps\n",
    "    max_epochs= 10, # Number of epochs to train for\n",
    "    gradient_clip_val= 1 # This is said to stabilize training, but we should test if that is true\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "56f72121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: torch.Size([10, 20])\n",
      "target: torch.Size([10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\will\\miniconda3\\envs\\Kettering\\Lib\\site-packages\\pytorch_lightning\\core\\module.py:441: You are trying to `self.log()` but the `self.trainer` reference is not registered on the model yet. This is most likely because the model hasn't been passed to the `Trainer`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(2.6261, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.validation_step(next(iter(dataLoaderTest)), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50140ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Before Training:\")\n",
    "test(model, testDataIn, testDataOut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5e231a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\will\\miniconda3\\envs\\Kettering\\Lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "\n",
      "  | Name     | Type             | Params | Mode \n",
      "------------------------------------------------------\n",
      "0 | model    | Modena           | 1.7 K  | train\n",
      "1 | lossFunc | CrossEntropyLoss | 0      | train\n",
      "------------------------------------------------------\n",
      "1.4 K     Trainable params\n",
      "280       Non-trainable params\n",
      "1.7 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "27        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "c:\\Users\\will\\miniconda3\\envs\\Kettering\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:420: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 3966/3966 [01:14<00:00, 53.40it/s, v_num=24, trainLoss=0.0463]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 3966/3966 [01:14<00:00, 53.40it/s, v_num=24, trainLoss=0.0463]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(learner, testingDataLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba8da65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(991441, 20)\n",
      "torch.Size([991441, 20])\n",
      "torch.Size([991441])\n",
      "tensor([13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n",
      "        13, 13])\n",
      "After Training:\n",
      "Model got 809364/991441 right. Accuracy of 81.63511494884719%\n",
      "43.89570332475659% Zeroes.\n"
     ]
    }
   ],
   "source": [
    "print(\"After Training:\")\n",
    "countR, tot, perc, percZero = test(model, testDataIn, testDataOut)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kettering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
