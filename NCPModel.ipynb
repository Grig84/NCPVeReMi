{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3842ba7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Imports:\n",
    "from ncps.wirings import AutoNCP\n",
    "from ncps.torch import CfC\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from numpy import genfromtxt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import matplotlib as plt\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "pl.seed_everything(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae4e3f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3160001, 12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\will\\AppData\\Local\\Temp\\ipykernel_20912\\3956869617.py:31: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:257.)\n",
      "  dataSet = torch.tensor(newData)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([623867, 10, 12])\n"
     ]
    }
   ],
   "source": [
    "#Dataset Formatting\n",
    "#Generate Time sequences that are 10 timepoints (Messages) with 7 features per message.\n",
    "#Organized by car.\n",
    "\n",
    "#Current Simulation File\n",
    "dataFile = 'Data/CfCMultiExtension/RandomPos_0709.csv' # DoS_0709\n",
    "\n",
    "dataSet = genfromtxt(dataFile, delimiter=',')\n",
    "batchSize = 64\n",
    "# Ceate dataloader and fill with (BSM, attk#). Expanding to add 0th dimension for batches.\n",
    "# Batch size should be 64 for the low density simulations and 128 for high density simulations.\n",
    "# No shuffle to keep batches on same vehicle.\n",
    "# Num_workers is set to = num CPU cores\n",
    "dataSet[0:-1,:] = dataSet[1:,:] # Get rid of the first null value of the dataset\n",
    "print(dataSet.shape)\n",
    "# count subsets per vehicle\n",
    "unq, counts = np.unique(dataSet[:, 2], return_counts = True)\n",
    "sender = 0\n",
    "lastSenderCount = 0\n",
    "newData = []\n",
    "# Organize dataset into sets of 10 messages by sender\n",
    "while sender < counts.shape[0]:\n",
    "    # Loop through sender\n",
    "    index = 0\n",
    "    while index < counts[sender] - 10:\n",
    "        # Loop through messages from sender\n",
    "        newData.append(dataSet[lastSenderCount+index:lastSenderCount +index+10])\n",
    "        index += 5\n",
    "    sender += 1\n",
    "    lastSenderCount += counts[sender-1]\n",
    "dataSet = torch.tensor(newData)\n",
    "leng = dataSet.shape[0]\n",
    "trainPerc = 80\n",
    "# Create new arrays per vehicle for federated learning\n",
    "splits = np.split(dataSet, np.cumsum(counts)[:-1])\n",
    "# Create seperate datasets for testing and training, using Train Percentage as metric for split\n",
    "trainDataIn = torch.Tensor(dataSet[:int(leng*(trainPerc/100)),:,3:10]).float() # 1\n",
    "trainDataOut = torch.Tensor(np.int_(dataSet[:int(leng*(trainPerc/100)),:,11])).long()\n",
    "testDataIn = torch.Tensor(dataSet[int(leng*(trainPerc/100)):,:,3:10]).float() # 1\n",
    "testDataOut = torch.Tensor(np.int_(dataSet[int(leng*(trainPerc/100)):,:,11])).long()\n",
    "newsetIn = []\n",
    "newsetOut = []\n",
    "testsetIn = []\n",
    "testsetOut = []\n",
    "# Create dataset of 1/100th of the entries for quicker testing during development\n",
    "for index in range(0,int(leng * (trainPerc/100))):\n",
    "    if not (int(index/10) % 10):\n",
    "        newsetIn.append(dataSet[index,:,3:10]) # 1\n",
    "        newsetOut.append((dataSet[index,:,11]))\n",
    "testingIn = torch.Tensor(np.array(newsetIn)).float()\n",
    "testingOut = torch.Tensor(np.array(newsetOut)).long()\n",
    "for idx in range(int((leng) * (trainPerc/100)), leng):\n",
    "    if not (int(idx/10) % 10):\n",
    "        testsetIn.append(dataSet[idx,:,3:10])\n",
    "        testsetOut.append((dataSet[idx,:,11]))\n",
    "testingIn = torch.Tensor(np.array(newsetIn)).float()\n",
    "testingOut = torch.Tensor(np.array(newsetOut)).long()\n",
    "inTest = torch.Tensor(np.array(testsetIn)).float()\n",
    "outTest = torch.Tensor(np.array(testsetOut)).long()\n",
    "# Create Dataloaders for all the datasets\n",
    "dataLoaderTrain = data.DataLoader(data.TensorDataset(trainDataIn, trainDataOut), batch_size=batchSize, shuffle=False, num_workers=10, persistent_workers = True, drop_last= True)\n",
    "dataLoaderTest = data.DataLoader(data.TensorDataset(testDataIn, testDataOut), batch_size=batchSize, shuffle=False, num_workers=10, persistent_workers = True, drop_last= True)\n",
    "testingDataLoader = data.DataLoader(data.TensorDataset(testingIn, testingOut), batch_size=batchSize, shuffle = False, num_workers=10, persistent_workers = True, drop_last= True)\n",
    "testingTestData = data.DataLoader(data.TensorDataset(testingIn, testingOut), batch_size=batchSize, shuffle = False, num_workers=10, persistent_workers = True, drop_last= True)\n",
    "print(dataSet.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b7f6e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutLogger():\n",
    "    def __init__(self, path):\n",
    "        #Helpers\n",
    "        self.path = path\n",
    "        self.epochTimes = []\n",
    "        self.times = []\n",
    "\n",
    "        #Outs\n",
    "        self.avgLossVEpoch = []\n",
    "        self.avgF1VEpoch = []\n",
    "        self.avgRecallVEpoch = []\n",
    "        self.avgPrecisionVEpoch = []\n",
    "        self.avgAccuracyVEpoch = []\n",
    "        self.lossVPercEvil = None\n",
    "        self.F1VPercEvil = None\n",
    "        self.RecallVPercEvil = None\n",
    "        self.PrecisionVPercEvil = None\n",
    "        self.AccuracyVPercEvil = None\n",
    "        self.AvgVehicleTime = None\n",
    "        self.MaxVehicleTime = None\n",
    "        self.TotTime = None\n",
    "\n",
    "    def startVehicleTimer(self):\n",
    "        self.startTime = time.time()\n",
    "    \n",
    "    def endVehicleTimer(self):\n",
    "        self.times.append(time.time()-self.startTime)\n",
    "\n",
    "    def startEpochTimer(self):\n",
    "        self.startEpochTime = time.time()\n",
    "    \n",
    "    def endEpochTimer(self):\n",
    "        self.epochTimes.append(time.time()-self.startEpochTime)\n",
    "\n",
    "    def updateLogs(self, vehicles, epoch):\n",
    "        currLoss = 0\n",
    "        currF1 = 0\n",
    "        currRecall = 0\n",
    "        currPrecision = 0\n",
    "        currAccuracy = 0\n",
    "        count = 0\n",
    "        for vehicle in vehicles:\n",
    "            currLoss += vehicle.curr_loss\n",
    "            f1, recall, precision, accuracy = vehicle.test(testDataIn, testDataOut, True)\n",
    "            currF1 += f1\n",
    "            currRecall += recall\n",
    "            currPrecision += precision\n",
    "            currAccuracy += accuracy\n",
    "            count += 1\n",
    "        self.avgLossVEpoch.append([epoch, currLoss/count])\n",
    "        self.avgF1VEpoch.append([epoch, currF1/count])\n",
    "        self.avgRecallVEpoch.append([epoch, currRecall/count])\n",
    "        self.avgPrecisionVEpoch.append([epoch, currPrecision/count])\n",
    "        self.avgAccuracyVEpoch.append([epoch, currAccuracy/count])\n",
    "            \n",
    "\n",
    "    def finalLogs(self, percEvil):\n",
    "        self.lossVPercEvil = [percEvil, self.avgLossVEpoch[-1][1]]\n",
    "        self.F1VPercEvil = [percEvil, self.avgF1VEpoch[-1][1]]\n",
    "        self.RecallVPercEvil = [percEvil, self.avgRecallVEpoch[-1][1]]\n",
    "        self.PrecisionVPercEvil = [percEvil, self.avgPrecisionVEpoch[-1][1]]\n",
    "        self.AccuracyVPercEvil = [percEvil, self.avgAccuracyVEpoch[-1][1]]\n",
    "        self.AvgVehicleTime = np.sum(self.times)/len(self.times)\n",
    "        self.MaxVehicleTime = np.max(self.times)\n",
    "        self.TotTime = np.sum(self.epochTimes)/len(self.epochTimes)\n",
    "\n",
    "    def log(self):\n",
    "        path = f\"out/{self.path}\"\n",
    "        if not os.path.exists(f\"out/{self.path}\"):\n",
    "            os.makedirs(f\"out/{self.path}\")\n",
    "        with open(f'{path}avgLossVEpoch.csv', 'w', newline='') as filename:\n",
    "            writer = csv.writer(filename)\n",
    "            writer.writerow(['epoch', 'avg Loss'])\n",
    "            writer.writerows(self.avgLossVEpoch)\n",
    "        with open(f'{path}avgF1VEpoch.csv', 'w', newline='') as filename:\n",
    "            writer = csv.writer(filename)\n",
    "            writer.writerow(['epoch', 'avg F1'])\n",
    "            writer.writerows(self.avgF1VEpoch)\n",
    "        with open(f'{path}avgRecallVEpoch.csv', 'w', newline='') as filename:\n",
    "            writer = csv.writer(filename)\n",
    "            writer.writerow(['epoch', 'avg Recall'])\n",
    "            writer.writerows(self.avgRecallVEpoch)\n",
    "        with open(f'{path}avgPrecisionVEpoch.csv', 'w', newline='') as filename:\n",
    "            writer = csv.writer(filename)\n",
    "            writer.writerow(['epoch', 'avg Precision'])\n",
    "            writer.writerows(self.avgPrecisionVEpoch)\n",
    "        with open(f'{path}avgAccuracyVEpoch.csv', 'w', newline='') as filename:\n",
    "            writer = csv.writer(filename)\n",
    "            writer.writerow(['epoch', 'avg Accuracy'])\n",
    "            writer.writerows(self.avgAccuracyVEpoch)\n",
    "        others = {'Loss V PercEvil':self.lossVPercEvil, 'F1 V PercEvil':self.F1VPercEvil, 'Recall V PercEvil':self.RecallVPercEvil, 'Precision V PercEvil':self.PrecisionVPercEvil, \n",
    "                  'Accuracy V PercEvil':self.AccuracyVPercEvil, 'Max Per-Vehicle Time':self.MaxVehicleTime, 'Avg Per-Vehicle Time':self.AvgVehicleTime, 'Total Time Per Epoch':self.TotTime}\n",
    "        with open(f'{path}avgAccuracyVEpoch.csv', 'w', newline='') as filename:\n",
    "            writer = csv.writer(filename)\n",
    "            writer.writerow(['epoch', 'avg Accuracy'])\n",
    "            writer.writerows(self.avgAccuracyVEpoch)\n",
    "        with open(f'{path}ExtraData.json', 'w') as filename:\n",
    "            json.dump(others, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f4ff8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Learner\n",
    "class CfCLearner(pl.LightningModule):\n",
    "    def __init__(self, model, lr):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.lossFunc = nn.CrossEntropyLoss()\n",
    "        self.loss = None\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Get in and out from batch\n",
    "        inputs, target = batch\n",
    "        # Put input through model\n",
    "        output, _ = self.model.forward(inputs)\n",
    "        # Reorganize inputs for use with loss function\n",
    "        output = output.permute(0, 2, 1)\n",
    "        # Calculate Loss using Cross Entropy Loss \n",
    "        loss = self.lossFunc(output, target)\n",
    "        self.log(\"trainLoss\", loss, prog_bar=True)\n",
    "        self.loss = loss\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # Get in and out from batch\n",
    "        inputs, target = batch\n",
    "        # Put input through model\n",
    "        output, _ = self.model.forward(inputs)\n",
    "        # Reorganize inputs for use with loss function\n",
    "        output = output.permute(0, 2, 1)\n",
    "        print(f\"output: {output.shape}\")\n",
    "        print(f\"target: {target.shape}\")\n",
    "        # Calculate Loss using Cross Entropy Loss \n",
    "        loss = self.lossFunc(output, target)\n",
    "        self.log(\"valLoss\", loss, prog_bar=True)\n",
    "        self.loss = loss\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self.validation_step(batch, batch_idx)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        # Using AdamW optomizer based on info from paper\n",
    "        # self.lr\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr = 0.001)\n",
    "        return ([optimizer], [torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.6)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28b52804",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Modena(nn.Module): \n",
    "    # CfC with feed-forward layer to classify at end.\n",
    "    def __init__(self, inputSize, unitNum, motorNum, outputDim, batchFirst = True):\n",
    "        super().__init__()\n",
    "        # Create NCP wiring for CfC\n",
    "        wiring = AutoNCP(unitNum, motorNum)\n",
    "        # Create CfC model with inputs and wiring\n",
    "        self.cfc = CfC(inputSize, wiring, batch_first=batchFirst)\n",
    "        # Create feed-forward layer\n",
    "        self.fF = nn.Linear(motorNum, outputDim)\n",
    "    \n",
    "    def forward(self, batch, hidden = None):\n",
    "        batch, hidden = self.cfc(batch, hidden) # Pass inputs through CfC\n",
    "        out = nn.functional.relu(self.fF(batch)) # pass through FeedForward Layer, then make 0 minimum\n",
    "        return out, hidden # Return the guess and the hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecad7b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OBU module class to organize\n",
    "class OBU():\n",
    "    def __init__(self, inputSize = 9, units = 20, motors = 8, outputs = 20, epochs = 0, lr = 0.001, gpu = False):\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.gpu = gpu\n",
    "        self.model = Modena(inputSize, units, motors, outputs)\n",
    "        self.learner = CfCLearner(self.model, lr) # tune units, lr\n",
    "        self.trainer = pl.Trainer(\n",
    "            logger = CSVLogger('log/Non-Fed'), # Set ouput destination of logs, logging accuracy every 50 steps\n",
    "            max_epochs = epochs, # Number of epochs to train for\n",
    "            gradient_clip_val = 1, # This is said to stabilize training, but we should test if that is true\n",
    "            accelerator = \"gpu\" if gpu else \"cpu\" # Using the GPU to run training or not\n",
    "            )\n",
    "        self.curr_loss = None\n",
    "    \n",
    "    def fit(self, dataLoader):\n",
    "        # calling built in fit function\n",
    "        self.trainer.fit(self.learner, dataLoader)\n",
    "        return self.learner.loss\n",
    "    \n",
    "    def step(self, epochs, dataLoader):\n",
    "        self.trainer.fit_loop.max_epochs = self.trainer.current_epoch + epochs\n",
    "        self.curr_loss = self.fit(dataLoader).item()\n",
    "    \n",
    "    def train(self, epochs, dataLoader, log):\n",
    "        epoch = 0\n",
    "        while epoch < epochs:\n",
    "            log.startEpochTimer()\n",
    "            log.startVehicleTimer()\n",
    "            self.step(1, dataLoader)\n",
    "            log.endEpochTimer()\n",
    "            log.endVehicleTimer()\n",
    "            log.updateLogs([self], epoch)\n",
    "            epoch += 1\n",
    "        log.finalLogs(0)\n",
    "        log.log()\n",
    "\n",
    "    \n",
    "    # Function to run model through a testing dataset and calculate accuracy. Can be expanded to give more metrics and more useful metrics.\n",
    "    def test(self, dataIn, dataOut, mathy = False):\n",
    "        # Put input data through model and determine classification\n",
    "        with torch.no_grad():\n",
    "            outs = np.asarray(self.model(dataIn)[0])\n",
    "        outs = torch.from_numpy(outs)\n",
    "        # Get the label with the maximum confidence for determining classification\n",
    "        print(outs.shape)\n",
    "        _, res = torch.max(outs, 2)\n",
    "        Pt = Pf = Nt = Nf = 0\n",
    "        countR = 0\n",
    "        numZero = 0\n",
    "        tot = outs.shape[0]\n",
    "        total = 0\n",
    "        for i in range(0, tot):\n",
    "            # Loop through sequences of 10 each\n",
    "            for t in range(0, res[i].shape[0]):\n",
    "                # Loop through the sub-sequences\n",
    "                if res[i,t] == dataOut[i,t]:\n",
    "                    if res[i,t] == 0:\n",
    "                        Nt += 1\n",
    "                        numZero += 1\n",
    "                    else:\n",
    "                        Pt += 1\n",
    "                    # Check if label is correct, and add to count right accordingly\n",
    "                    countR += 1\n",
    "                else:\n",
    "                    if dataOut[i,t] == 0:\n",
    "                        Pf += 1\n",
    "                        numZero += 1\n",
    "                    else:\n",
    "                        Nf += 1\n",
    "                total += 1\n",
    "        # Calculate percent correct and percent zero\n",
    "        if mathy:\n",
    "            if Pt != 0:\n",
    "                accuracy = (Pt+Nt)/(Pt+Pf+Nf+Nt)\n",
    "                precision = (Pt)/(Pt+Pf)\n",
    "                recall = (Pt)/(Pt+Nf)\n",
    "                f1 = (2*precision*recall)/(precision+recall)\n",
    "                print(precision)\n",
    "                print(recall)\n",
    "                print(\"Model got \" + str(countR) + \"/\" + str(total) + \" right.\")\n",
    "                print(f\"Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1 Score: {f1}\")\n",
    "                print(f\"{numZero}, {numZero/total * 100}% Zeroes, {total-numZero} Non Zero entries.\")\n",
    "                return f1, recall, precision, accuracy\n",
    "            else:\n",
    "                print(\"Model could not complete tests.\")\n",
    "                return 0, 0, 0, 0\n",
    "        else:\n",
    "            if Pt != 0:\n",
    "                accuracy = (Pt+Nt)/(Pt+Pf+Nf+Nt)\n",
    "                precision = (Pt)/(Pt+Pf)\n",
    "                recall = (Pt)/(Pt+Nf)\n",
    "                f1 = (2*precision*recall)/(precision+recall)\n",
    "                print(precision)\n",
    "                print(recall)\n",
    "                print(\"Model got \" + str(countR) + \"/\" + str(total) + \" right.\")\n",
    "                print(f\"Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1 Score: {f1}\")\n",
    "                print(f\"{numZero}, {numZero/total * 100}% Zeroes, {total-numZero} Non Zero entries.\")\n",
    "                return f\"Model got {countR}/{total} right. Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1 Score: {f1}\"\n",
    "            else:\n",
    "                print(\"Model could not complete tests.\")\n",
    "                return f\"Model could not complete tests, found 0 of misbehaviour.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f62e03e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "lr = 0.01\n",
    "testOBU = OBU(\n",
    "    inputSize = 7, # 9  # Number of features per BSM\n",
    "    units = 20, # Number of hidden cells\n",
    "    motors = 8, # Number of motor neurons\n",
    "    outputs = 20, # Number of possible labels\n",
    "    lr = lr, # 0.001\n",
    "    gpu = False\n",
    ")\n",
    "path = f\"Normal/{epochs}-{lr}/\"\n",
    "\n",
    "log = OutLogger(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56f72121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: torch.Size([64, 20, 10])\n",
      "target: torch.Size([64, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/will/.conda/envs/Kettering/lib/python3.13/site-packages/pytorch_lightning/core/module.py:441: You are trying to `self.log()` but the `self.trainer` reference is not registered on the model yet. This is most likely because the model hasn't been passed to the `Trainer`\n"
     ]
    }
   ],
   "source": [
    "testOBU.testStep(dataLoaderTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b50140ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Training:\n",
      "torch.Size([196662, 10, 20])\n",
      "Model got 0/1966620 right. Accuracy of 0.0%\n",
      "43.690189258728175% Zeroes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 1966620, 0.0, 43.690189258728175)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Before Training:\")\n",
    "testOBU.test(testDataIn, testDataOut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e231a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\will\\miniconda3\\envs\\Kettering\\Lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "\n",
      "  | Name     | Type             | Params | Mode \n",
      "------------------------------------------------------\n",
      "0 | model    | Modena           | 1.6 K  | train\n",
      "1 | lossFunc | CrossEntropyLoss | 0      | train\n",
      "------------------------------------------------------\n",
      "1.3 K     Trainable params\n",
      "264       Non-trainable params\n",
      "1.6 K     Total params\n",
      "0.006     Total estimated model params size (MB)\n",
      "27        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 7798/7798 [05:07<00:00, 25.35it/s, v_num=37, trainLoss=0.00192]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 7798/7798 [05:07<00:00, 25.34it/s, v_num=37, trainLoss=0.00192]\n",
      "torch.Size([124774, 10, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\will\\miniconda3\\envs\\Kettering\\Lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:654: Checkpoint directory log/Non-Fed\\lightning_logs\\version_37\\checkpoints exists and is not empty.\n",
      "\n",
      "  | Name     | Type             | Params | Mode \n",
      "------------------------------------------------------\n",
      "0 | model    | Modena           | 1.6 K  | train\n",
      "1 | lossFunc | CrossEntropyLoss | 0      | train\n",
      "------------------------------------------------------\n",
      "1.3 K     Trainable params\n",
      "264       Non-trainable params\n",
      "1.6 K     Total params\n",
      "0.006     Total estimated model params size (MB)\n",
      "27        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9800511481613118\n",
      "0.9892962153705053\n",
      "Model got 1236307/1247740 right.\n",
      "Accuracy: 0.9908370333563082, Precision: 0.9800511481613118, Recall: 0.9892962153705053, F1 Score: 0.984651981361682\n",
      "877030, 70.28948338596182% Zeroes, 370710 Non Zero entries.\n",
      "Epoch 1: 100%|██████████| 7798/7798 [03:32<00:00, 36.65it/s, v_num=37, trainLoss=0.0013]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 7798/7798 [03:32<00:00, 36.64it/s, v_num=37, trainLoss=0.0013]\n",
      "torch.Size([124774, 10, 20])\n",
      "0.9713774314135463\n",
      "0.9925548272234361\n",
      "Model got 1234138/1247740 right.\n",
      "Accuracy: 0.9890986904323016, Precision: 0.9713774314135463, Recall: 0.9925548272234361, F1 Score: 0.9818519496945973\n",
      "877030, 70.28948338596182% Zeroes, 370710 Non Zero entries.\n"
     ]
    }
   ],
   "source": [
    "# Training dataset - much larger, but more accurate\n",
    "\n",
    "testOBU.train(epochs, dataLoaderTrain, log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61352918",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/Kettering/lib/python3.13/site-packages/pytorch_lightning/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "\n",
      "  | Name     | Type             | Params | Mode \n",
      "------------------------------------------------------\n",
      "0 | model    | Modena           | 1.6 K  | train\n",
      "1 | lossFunc | CrossEntropyLoss | 0      | train\n",
      "------------------------------------------------------\n",
      "1.3 K     Trainable params\n",
      "264       Non-trainable params\n",
      "1.6 K     Total params\n",
      "0.006     Total estimated model params size (MB)\n",
      "27        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/opt/anaconda3/envs/Kettering/lib/python3.13/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:420: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 780/780 [00:45<00:00, 17.09it/s, v_num=8, trainLoss=0.0105] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 780/780 [00:45<00:00, 17.08it/s, v_num=8, trainLoss=0.0105]\n"
     ]
    }
   ],
   "source": [
    "# Small subset of training dataset (10%) designed for test running the training\n",
    "testOBU.fit(testingDataLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ba8da65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Training:\n",
      "torch.Size([124774, 10, 20])\n",
      "0.9713774314135463\n",
      "0.9925548272234361\n",
      "Model got 1234138/1247740 right.\n",
      "Accuracy: 0.9890986904323016, Precision: 0.9713774314135463, Recall: 0.9925548272234361, F1 Score: 0.9818519496945973\n",
      "877030, 70.28948338596182% Zeroes, 370710 Non Zero entries.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Model got 1234138/1247740 right. Accuracy: 0.9890986904323016, Precision: 0.9713774314135463, Recall: 0.9925548272234361, F1 Score: 0.9818519496945973'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"After Training:\")\n",
    "testOBU.test(testDataIn, testDataOut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0af50b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([49910, 10, 20])\n",
      "0.9742246122243374\n",
      "0.9941188251001335\n",
      "Model got 494279/499100 right.\n",
      "Accuracy: 0.9903406131035865, Precision: 0.9742246122243374, Recall: 0.9941188251001335, F1 Score: 0.9840711824198191\n",
      "349300, 69.9859747545582% Zeroes, 149800 Non Zero entries.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Model got 494279/499100 right. Accuracy: 0.9903406131035865, Precision: 0.9742246122243374, Recall: 0.9941188251001335, F1 Score: 0.9840711824198191'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testOBU.test(testingIn, testingOut)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kettering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
