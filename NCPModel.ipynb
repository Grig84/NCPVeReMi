{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3842ba7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports:\n",
    "from ncps.wirings import AutoNCP\n",
    "from ncps.torch import CfC\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from numpy import genfromtxt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import matplotlib as plt\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4e3f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4957201, 12)\n"
     ]
    }
   ],
   "source": [
    "#PROPER FORMATTING\n",
    "#Time sequences are 10 timepoints (Messages) with 7 features per message.\n",
    "#Organized by car.\n",
    "\n",
    "#Current Simulation File\n",
    "dataFile = 'Data/CfCMultiExtension/DoS_0709.csv'\n",
    "\n",
    "dataSet = genfromtxt(dataFile, delimiter=',')\n",
    "batchSize = 64\n",
    "# Ceate dataloader and fill with (BSM, attk#). Expanding to add 0th dimension for batches.\n",
    "# Batch size should be 64 for the low density simulations and 128 for high density simulations.\n",
    "# No shuffle to keep batches on same vehicle.\n",
    "# Num_workers is set to = num CPU cores\n",
    "dataSet[0:-1,:] = dataSet[1:,:] # Get rid of the first null value of the dataset\n",
    "print(dataSet.shape)\n",
    "# count subsets per vehicle\n",
    "unq, counts = np.unique(dataSet[:, 2], return_counts = True)\n",
    "# Create new arrays per vehicle for federated learning\n",
    "splits = np.split(dataSet, np.cumsum(counts)[:-1])\n",
    "sender = 0\n",
    "lastSenderCount = 0\n",
    "newData = []\n",
    "# Organize dataset into sets of 10 messages by sender\n",
    "while sender < counts.shape[0]:\n",
    "    # Loop through sender\n",
    "    index = 0\n",
    "    while index < counts[sender] - 10:\n",
    "        # Loop through messages from sender\n",
    "        newData.append(dataSet[lastSenderCount+index:lastSenderCount +index+10])\n",
    "        index += 5\n",
    "    sender += 1\n",
    "    lastSenderCount = counts[sender-1]\n",
    "dataSet = torch.tensor(newData)\n",
    "len = dataSet.shape[0]\n",
    "trainPerc = 80\n",
    "# Create seperate datasets for testing and training, using Train Percentage as metric for split\n",
    "trainDataIn = torch.Tensor(dataSet[:int(len*(trainPerc/100)),:,1:10]).float()\n",
    "trainDataOut = torch.Tensor(np.int_(dataSet[:int(len*(trainPerc/100)),:,11])).long()\n",
    "testDataIn = torch.Tensor(dataSet[int(len*(trainPerc/100)):,:,1:10]).float()\n",
    "testDataOut = torch.Tensor(np.int_(dataSet[int(len*(trainPerc/100)):,:,11])).long()\n",
    "newsetIn = []\n",
    "newsetOut = []\n",
    "# Create dataset of 1/100th of the entries for quicker testing during development\n",
    "for index in range(0,int(len * (trainPerc/100))):\n",
    "    if not (int(index/10) % 100):\n",
    "        newsetIn.append(dataSet[index,:,1:10])\n",
    "        newsetOut.append((dataSet[index,:,11]))\n",
    "testingIn = torch.Tensor(np.array(newsetIn)).float()\n",
    "testingOut = torch.Tensor(np.array(newsetOut)).long()\n",
    "# Create Dataloaders for all the datasets\n",
    "dataLoaderTrain = data.DataLoader(data.TensorDataset(trainDataIn, trainDataOut), batch_size=batchSize, shuffle=False, num_workers=16)\n",
    "dataLoaderTest = data.DataLoader(data.TensorDataset(testDataIn, testDataOut), batch_size=batchSize, shuffle=False, num_workers=16)\n",
    "testingDataLoader = data.DataLoader(data.TensorDataset(testingIn, testingOut), batch_size=batchSize, shuffle = False, num_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "55f9ff02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3965760, 9])\n",
      "torch.Size([3965760])\n",
      "torch.Size([39660])\n",
      "torch.Size([39660, 9])\n",
      "tensor([1.9347e+04, 1.9611e+04, 3.1161e+04, 6.6291e+01, 2.4601e+02, 3.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00])\n",
      "tensor(0)\n",
      "tensor([1.9347e+04, 1.9611e+04, 3.1161e+04, 6.6291e+01, 2.4601e+02, 4.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00])\n",
      "tensor(0)\n",
      "[[0.00000000e+00 4.50000000e+01 9.00000000e+00 2.52126029e+04\n",
      "  7.93602911e+00 9.57097011e+01 1.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 4.50000000e+01 9.00000000e+00 2.52136029e+04\n",
      "  1.32969123e+01 1.06405701e+02 1.00000000e+00 2.28293295e+00\n",
      "  1.43427328e+01 2.66527912e-01 5.15145214e-01 0.00000000e+00]]\n",
      "(12,)\n"
     ]
    }
   ],
   "source": [
    "print(trainDataIn.shape)\n",
    "print(trainDataOut.shape)\n",
    "print(testingOut.shape)\n",
    "print(testingIn.shape)\n",
    "print(trainDataIn[-1])\n",
    "print(trainDataOut[1])\n",
    "print(testDataIn[15])\n",
    "print(testingOut[0])\n",
    "print(dataSet[0:2])\n",
    "print(dataSet[90].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4ff8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Learner\n",
    "class CfCLearner(pl.LightningModule):\n",
    "    def __init__(self, model, lr):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.lossFunc = nn.CrossEntropyLoss()\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Get in and out from batch\n",
    "        inputs, target = batch\n",
    "        # Put input through model\n",
    "        output, _ = self.model.forward(inputs)\n",
    "        # Reorganize inputs for use with loss function\n",
    "        output = output.permute(0, 2, 1)\n",
    "        # Calculate Loss using Cross Entropy Loss \n",
    "        loss = self.lossFunc(output, target)\n",
    "        self.log(\"trainLoss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # Get in and out from batch\n",
    "        inputs, target = batch\n",
    "        # Put input through model\n",
    "        output, _ = self.model.forward(inputs)\n",
    "        # Reorganize inputs for use with loss function\n",
    "        output = output.permute(0, 2, 1)\n",
    "        print(f\"output: {output.shape}\")\n",
    "        print(f\"target: {target.shape}\")\n",
    "        # Calculate Loss using Cross Entropy Loss \n",
    "        loss = self.lossFunc(output, target)\n",
    "        self.log(\"valLoss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self.validation_step(batch, batch_idx)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Using AdamW optomizer based on info from paper\n",
    "        return torch.optim.AdamW(self.model.parameters(), lr = self.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "28b52804",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Modena(nn.Module): \n",
    "    # CfC with feed-forward layer to classify at end.\n",
    "    def __init__(self, inputSize, unitNum, motorNum, outputDim, batchFirst = True):\n",
    "        super().__init__()\n",
    "        # Create NCP wiring for CfC\n",
    "        wiring = AutoNCP(unitNum, motorNum)\n",
    "        # Create CfC model with inputs and wiring\n",
    "        self.cfc = CfC(inputSize, wiring, batch_first=batchFirst)\n",
    "        # Create feed-forward layer\n",
    "        self.fF = nn.Linear(motorNum, outputDim)\n",
    "    \n",
    "    def forward(self, batch, hidden = None):\n",
    "        batch, hidden = self.cfc(batch, hidden) # Pass inputs through CfC\n",
    "        out = nn.functional.relu(self.fF(batch)) # pass through FeedForward Layer, then make 0 minimum\n",
    "        return out, hidden # Return the guess and the hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38b459e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run model through a testing dataset and calculate accuracy. Can be expanded to give more metrics and more useful metrics.\n",
    "def test(model, dataIn, dataOut):\n",
    "    # Put input data through model and determine classification\n",
    "    with torch.no_grad():\n",
    "        outs = np.asarray(model(dataIn)[0])\n",
    "    outs = torch.from_numpy(outs)\n",
    "    # Get the label with the maximum confidence for determining classification\n",
    "    _, res = torch.max(outs, 2)\n",
    "    countR = 0\n",
    "    numZero = 0\n",
    "    tot = outs.shape[0]\n",
    "    total = 0\n",
    "    for i in range(0, tot):\n",
    "        # Loop through sequences of 10 each\n",
    "        for t in range(0, res[i].shape[0]):\n",
    "            # Loop through the sub-sequences\n",
    "            if res[i,t] == dataOut[i,t]:\n",
    "                # Check if label is correct, and add to count right accordingly\n",
    "                countR += 1\n",
    "            if dataOut[i,t] == 0:\n",
    "                # If the label is zero, increment the count of zeroes to determine if model is just outputting zeroes\n",
    "                numZero += 1\n",
    "            total += 1\n",
    "    # Calculate percent correct and percent zero\n",
    "    perc = (countR/total) * 100\n",
    "    percZero = (numZero/total) * 100\n",
    "    print(\"Model got \" + str(countR) + \"/\" + str(total) + \" right. Accuracy of \" + str(perc) + \"%\")\n",
    "    print(str(percZero) + \"% Zeroes.\")\n",
    "    return countR, total, perc, percZero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f62e03e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "#20 units, 8 motor neuron (output)\n",
    "\n",
    "#input size = 9, 20 units, 8 motor neurons, and 20 possible outputs.\n",
    "model = Modena(9, 20, 8, 20)\n",
    "learner = CfCLearner(model, lr=0.001) #Tune units, lr\n",
    "trainer = pl.Trainer(\n",
    "    logger = CSVLogger('log'), # Set ouput destination of logs, logging accuracy every 50 steps\n",
    "    max_epochs= 10, # Number of epochs to train for\n",
    "    gradient_clip_val= 1 # This is said to stabilize training, but we should test if that is true\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "56f72121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: torch.Size([64, 10, 20])\n",
      "target: torch.Size([64, 10])\n",
      "output: torch.Size([64, 20, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\will\\miniconda3\\envs\\Kettering\\Lib\\site-packages\\pytorch_lightning\\core\\module.py:441: You are trying to `self.log()` but the `self.trainer` reference is not registered on the model yet. This is most likely because the model hasn't been passed to the `Trainer`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(2.9553, grad_fn=<NllLoss2DBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.validation_step(next(iter(dataLoaderTest)), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b50140ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Training:\n",
      "Model got 0/1966620 right. Accuracy of 0.0%\n",
      "39.65916140382993% Zeroes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 1966620, 0.0, 39.65916140382993)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Before Training:\")\n",
    "test(model, testDataIn, testDataOut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5e231a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type             | Params | Mode \n",
      "------------------------------------------------------\n",
      "0 | model    | Modena           | 1.7 K  | train\n",
      "1 | lossFunc | CrossEntropyLoss | 0      | train\n",
      "------------------------------------------------------\n",
      "1.4 K     Trainable params\n",
      "280       Non-trainable params\n",
      "1.7 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "27        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 123/123 [00:22<00:00,  5.52it/s, v_num=26, trainLoss=0.257]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 123/123 [00:22<00:00,  5.51it/s, v_num=26, trainLoss=0.257]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(learner, testingDataLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6ba8da65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Training:\n",
      "Model got 1666292/1966620 right. Accuracy of 84.72872237646317%\n",
      "39.65916140382993% Zeroes.\n"
     ]
    }
   ],
   "source": [
    "print(\"After Training:\")\n",
    "countR, tot, perc, percZero = test(model, testDataIn, testDataOut)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kettering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
