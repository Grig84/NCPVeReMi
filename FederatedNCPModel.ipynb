{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a997a78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports:\n",
    "from ncps.wirings import AutoNCP\n",
    "from ncps.torch import CfC\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from numpy import genfromtxt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import matplotlib as plt\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.set_float32_matmul_precision(\"high\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2491f35d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4957201, 12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\will\\AppData\\Local\\Temp\\ipykernel_19308\\565797722.py:31: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:257.)\n",
      "  dataSet = torch.tensor(newData)\n"
     ]
    }
   ],
   "source": [
    "# Load Base File\n",
    "#Current Simulation File\n",
    "dataFile = 'Data/CfCMultiExtension/DoS_0709.csv'\n",
    "\n",
    "dataSet = genfromtxt(dataFile, delimiter=',')\n",
    "\n",
    "batchSize = 64\n",
    "# Ceate dataloader and fill with (BSM, attk#). Expanding to add 0th dimension for batches.\n",
    "# Batch size should be 64 for the low density simulations and 128 for high density simulations.\n",
    "# No shuffle to keep batches on same vehicle.\n",
    "# Num_workers is set to = num CPU cores\n",
    "dataSet[0:-1,:] = dataSet[1:,:] # Get rid of the first null value of the dataset\n",
    "# Sort Dataset by reciever id to pass on to every car.\n",
    "dataSet = dataSet[np.argsort(dataSet[:, 1])]\n",
    "print(dataSet.shape)\n",
    "# count subsets per vehicle\n",
    "unq, counts = np.unique(dataSet[:, 1], return_counts = True)\n",
    "recvr = 0\n",
    "lastRecieverCount = 0\n",
    "newData = []\n",
    "# Organize dataset into sets of 10 messages by reciever\n",
    "while recvr < counts.shape[0]:\n",
    "    # Loop through reciever\n",
    "    index = 0\n",
    "    while index < counts[recvr] - 10:\n",
    "        # Loop through messages from reciever\n",
    "        newData.append(dataSet[lastRecieverCount+index:lastRecieverCount +index+10])\n",
    "        index += 5\n",
    "    recvr += 1\n",
    "    lastRecieverCount = counts[recvr-1]\n",
    "dataSet = torch.tensor(newData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65c814c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PROPER FORMATTING\n",
    "#Time sequences are 10 timepoints (Messages) with 7 features per message.\n",
    "#Organized by car.\n",
    "\n",
    "len = dataSet.shape[0]\n",
    "trainPerc = 80\n",
    "# Create new arrays per vehicle for federated learning\n",
    "splits = np.split(dataSet, np.cumsum(counts)[:-1])\n",
    "# Create seperate datasets for testing and training, using Train Percentage as metric for split\n",
    "trainDataIn = torch.Tensor(dataSet[:int(len*(trainPerc/100)),:,1:10]).float()\n",
    "trainDataOut = torch.Tensor(np.int_(dataSet[:int(len*(trainPerc/100)),:,11])).long()\n",
    "testDataIn = torch.Tensor(dataSet[int(len*(trainPerc/100)):,:,1:10]).float()\n",
    "testDataOut = torch.Tensor(np.int_(dataSet[int(len*(trainPerc/100)):,:,11])).long()\n",
    "newsetIn = []\n",
    "newsetOut = []\n",
    "testsetIn = []\n",
    "testsetOut = []\n",
    "# Create dataset of 1/100th of the entries for quicker testing during development\n",
    "for index in range(0,int((len) * (trainPerc/100))):\n",
    "    if not (int(index/10) % 100):\n",
    "        newsetIn.append(dataSet[index,:,1:10])\n",
    "        newsetOut.append((dataSet[index,:,11]))\n",
    "for idx in range(int((len) * (trainPerc/100)), len):\n",
    "    if not (int(idx/10) % 10):\n",
    "        testsetIn.append(dataSet[idx,:,1:10])\n",
    "        testsetOut.append((dataSet[idx,:,11]))\n",
    "testingIn = torch.Tensor(np.array(newsetIn)).float()\n",
    "testingOut = torch.Tensor(np.array(newsetOut)).long()\n",
    "inTest = torch.Tensor(np.array(testsetIn)).float()\n",
    "outTest = torch.Tensor(np.array(testsetOut)).long()\n",
    "# Create Dataloaders for all the datasets\n",
    "dataLoaderTrain = data.DataLoader(data.TensorDataset(trainDataIn, trainDataOut), batch_size=batchSize, shuffle=False, num_workers=16, persistent_workers = True, drop_last= True)\n",
    "dataLoaderTest = data.DataLoader(data.TensorDataset(testDataIn, testDataOut), batch_size=batchSize, shuffle=False, num_workers=16, persistent_workers = True, drop_last= True)\n",
    "testingDataLoader = data.DataLoader(data.TensorDataset(testingIn, testingOut), batch_size=batchSize, shuffle = False, num_workers=16, persistent_workers = True, drop_last= True)\n",
    "testingTestData = data.DataLoader(data.TensorDataset(testingIn, testingOut), batch_size=batchSize, shuffle = False, num_workers=16, persistent_workers = True, drop_last= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b158270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([19700, 10, 9])\n",
      "torch.Size([19700, 10])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inTest.shape)\n",
    "print(outTest.shape)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1da521c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3 4]\n",
      " [9 8 7 6]\n",
      " [5 7 8 3]\n",
      " [9 8 7 3]]\n",
      "[[1 2 3 4]\n",
      " [9 8 7 6]\n",
      " [9 8 7 3]\n",
      " [5 7 8 3]]\n"
     ]
    }
   ],
   "source": [
    "#TESTING\n",
    "tom = np.array([[1,2,3,4],[9,8,7,6],[5,7,8,3],[9,8,7,3]])\n",
    "print(tom)\n",
    "tom = tom[np.argsort(tom[:, 2])]\n",
    "print(tom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "01abeb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Learner\n",
    "class CfCLearner(pl.LightningModule):\n",
    "    def __init__(self, model, lr):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.lossFunc = nn.CrossEntropyLoss()\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Get in and out from batch\n",
    "        inputs, target = batch\n",
    "        # Put input through model\n",
    "        output, _ = self.model.forward(inputs)\n",
    "        # Reorganize inputs for use with loss function\n",
    "        output = output.permute(0, 2, 1)\n",
    "        # Calculate Loss using Cross Entropy Loss \n",
    "        loss = self.lossFunc(output, target)\n",
    "        self.log(\"trainLoss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # Get in and out from batch\n",
    "        inputs, target = batch\n",
    "        # Put input through model\n",
    "        output, _ = self.model.forward(inputs)\n",
    "        # Reorganize inputs for use with loss function\n",
    "        output = output.permute(0, 2, 1)\n",
    "        print(f\"output: {output.shape}\")\n",
    "        print(f\"target: {target.shape}\")\n",
    "        # Calculate Loss using Cross Entropy Loss \n",
    "        loss = self.lossFunc(output, target)\n",
    "        self.log(\"valLoss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self.validation_step(batch, batch_idx)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Using AdamW optomizer based on info from paper\n",
    "        return torch.optim.AdamW(self.model.parameters(), lr = self.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bfb6cf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Modena(nn.Module): \n",
    "    # CfC with feed-forward layer to classify at end.\n",
    "    def __init__(self, inputSize, unitNum = None, motorNum = 2, outputDim = 2, batchFirst = True):\n",
    "        super().__init__()\n",
    "        if isinstance(inputSize, Modena):\n",
    "            self.inputSize = inputSize.inputSize\n",
    "            self.unitNum = inputSize.unitNum\n",
    "            self.motorNum = inputSize.motorNum\n",
    "            self.outputDim = inputSize.outputDim\n",
    "            self.batchFirst = inputSize.batchFirst\n",
    "            # Create NCP wiring for CfC\n",
    "            wiring = AutoNCP(self.unitNum, self.motorNum)\n",
    "            # Create CfC model with inputs and wiring\n",
    "            self.cfc = CfC(self.inputSize, wiring, batch_first=self.batchFirst)\n",
    "            # Create feed-forward layer\n",
    "            self.fF = nn.Linear(self.motorNum, self.outputDim)\n",
    "            self.fF.weight = nn.Parameter(inputSize.fF.weight)\n",
    "        else:\n",
    "            self.inputSize = inputSize\n",
    "            self.unitNum = unitNum\n",
    "            self.motorNum = motorNum\n",
    "            self.outputDim = outputDim\n",
    "            self.batchFirst = batchFirst\n",
    "            # Create NCP wiring for CfC\n",
    "            wiring = AutoNCP(unitNum, motorNum)\n",
    "            # Create CfC model with inputs and wiring\n",
    "            self.cfc = CfC(inputSize, wiring, batch_first=batchFirst)\n",
    "            # Create feed-forward layer\n",
    "            self.fF = nn.Linear(motorNum, outputDim)\n",
    "        \n",
    "\n",
    "    def forward(self, batch, hidden = None):\n",
    "        batch, hidden = self.cfc(batch, hidden) # Pass inputs through CfC\n",
    "        out = nn.functional.relu(self.fF(batch)) # pass through FeedForward Layer, then make 0 minimum\n",
    "        return out, hidden # Return the guess and the hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5406ad4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OBU module class to organize\n",
    "class OBU():\n",
    "    def __init__(self, inputSize, units = 20, motors = 8, outputs = 20, epochs = 10, lr = 0.001, gpu = False):\n",
    "        if isinstance(inputSize, OBU):\n",
    "            self.lr = inputSize.lr\n",
    "            self.epochs = inputSize.epochs\n",
    "            self.gpu = inputSize.gpu\n",
    "            self.model = Modena(inputSize.model)\n",
    "            self.model.load_state_dict(inputSize.model.state_dict())\n",
    "            self.learner = CfCLearner(self.model, self.lr)\n",
    "            self.trainer = pl.Trainer(\n",
    "                logger = CSVLogger('log'), # Set ouput destination of logs, logging accuracy every 50 steps\n",
    "                max_epochs = self.epochs, # Number of epochs to train for\n",
    "                gradient_clip_val = 1, # This is said to stabilize training, but we should test if that is true\n",
    "                accelerator = \"gpu\" if self.gpu else \"cpu\" # Using the GPU to run training or not\n",
    "                )\n",
    "        else:\n",
    "            self.lr = lr\n",
    "            self.epochs = epochs\n",
    "            self.gpu = gpu\n",
    "            self.model = Modena(inputSize, units, motors, outputs)\n",
    "            self.learner = CfCLearner(self.model, lr) # tune units, lr\n",
    "            self.trainer = pl.Trainer(\n",
    "                logger = CSVLogger('log'), # Set ouput destination of logs, logging accuracy every 50 steps\n",
    "                max_epochs = epochs, # Number of epochs to train for\n",
    "                gradient_clip_val = 1, # This is said to stabilize training, but we should test if that is true\n",
    "                accelerator = \"gpu\" if gpu else \"cpu\" # Using the GPU to run training or not\n",
    "                )\n",
    "    \n",
    "        # Overloading add function to create fed.avg. model\n",
    "    def __add__(self, other):\n",
    "        # if self.learner.getHidden() != None and other.learner.getHidden() != None:\n",
    "        self.model.load_state_dict(dict( (n, self.model.state_dict().get(n, 0)+other.model.state_dict().get(n, 0)) for n in set(self.model.state_dict())|set(other.model.state_dict()) ))\n",
    "        # elif other.learner.getHidden() != None:\n",
    "        #     self.model.load_state_dict(other.model.state_dict())\n",
    "        # elif self.learner.getHidden() != None:\n",
    "        #     self.model.load_state_dict(self.model.state_dict())\n",
    "        return self\n",
    "\n",
    "    # Overloading div. function to average model\n",
    "    def __truediv__(self, i):\n",
    "        \n",
    "        self.model.load_state_dict(dict((n, self.model.state_dict().get(n, 0)/i) for n in self.model.state_dict()))\n",
    "        # self.model.load_state_dict(self.model.state_dict()/i)\n",
    "        # self.learner.setHidden(self.learner.getHidden() / i)\n",
    "        # self.model.fF.weight = nn.Parameter(self.model.fF.weight/i)\n",
    "        return self\n",
    "    \n",
    "    def fit(self, dataLoader):\n",
    "        # calling built in fit function\n",
    "        return self.trainer.fit(self.learner, dataLoader)\n",
    "    \n",
    "    # Function to run model through a testing dataset and calculate accuracy. Can be expanded to give more metrics and more useful metrics.\n",
    "    def test(self, dataIn, dataOut, extraLayer = True):\n",
    "        # Put input data through model and determine classification\n",
    "        with torch.no_grad():\n",
    "            outs = self.model(dataIn)\n",
    "        if extraLayer:\n",
    "            outs = outs[0]\n",
    "        outs = np.asarray(outs)\n",
    "        outs = torch.from_numpy(outs)\n",
    "        # Get the label with the maximum confidence for determining classification\n",
    "        print(outs.shape)\n",
    "        _, res = torch.max(outs, 2)\n",
    "        countR = 0\n",
    "        numZero = 0\n",
    "        tot = outs.shape[0]\n",
    "        total = 0\n",
    "        for i in range(0, tot):\n",
    "            # Loop through sequences of 10 each\n",
    "            for t in range(0, res[i].shape[0]):\n",
    "                # Loop through the sub-sequences\n",
    "                if res[i,t] == dataOut[i,t]:\n",
    "                    # Check if label is correct, and add to count right accordingly\n",
    "                    countR += 1\n",
    "                if dataOut[i,t] == 0:\n",
    "                    # If the label is zero, increment the count of zeroes to determine if model is just outputting zeroes\n",
    "                    numZero += 1\n",
    "                total += 1\n",
    "        # Calculate percent correct and percent zero\n",
    "        perc = (countR/total) * 100\n",
    "        percZero = (numZero/total) * 100\n",
    "        print(\"Model got \" + str(countR) + \"/\" + str(total) + \" right. Accuracy of \" + str(perc) + \"%\")\n",
    "        print(str(percZero) + \"% Zeroes.\")\n",
    "        return countR, total, perc, percZero\n",
    "    \n",
    "    def testStep(self, dataLoader):\n",
    "        self.learner.validation_step(next(iter(dataLoader)), 0)\n",
    "    \n",
    "    def setModel(self, model):\n",
    "        if not model == None:\n",
    "            self.model = model\n",
    "\n",
    "    def getModel(self):\n",
    "        return self.model\n",
    "    \n",
    "    def resetTrainer(self):\n",
    "        self.trainer = pl.Trainer(\n",
    "            logger = CSVLogger('log'), # Set ouput destination of logs, logging accuracy every 50 steps\n",
    "            max_epochs = self.epochs, # Number of epochs to train for\n",
    "            gradient_clip_val = 1, # This is said to stabilize training, but we should test if that is true\n",
    "            accelerator = \"gpu\" if self.gpu else \"cpu\" # Using the GPU to run training or not\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd58282c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\will\\miniconda3\\envs\\Kettering\\Lib\\site-packages\\pytorch_lightning\\trainer\\setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\will\\miniconda3\\envs\\Kettering\\Lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "\n",
      "  | Name     | Type             | Params | Mode \n",
      "------------------------------------------------------\n",
      "0 | model    | Modena           | 1.7 K  | train\n",
      "1 | lossFunc | CrossEntropyLoss | 0      | train\n",
      "------------------------------------------------------\n",
      "1.4 K     Trainable params\n",
      "280       Non-trainable params\n",
      "1.7 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "27        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "c:\\Users\\will\\miniconda3\\envs\\Kettering\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:310: The number of training batches (11) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:  91%|█████████ | 10/11 [00:00<00:00, 41.43it/s, v_num=859, trainLoss=2.780]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\will\\miniconda3\\envs\\Kettering\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:48\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
      "File \u001b[1;32mc:\\Users\\will\\miniconda3\\envs\\Kettering\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:599\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    593\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[0;32m    594\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[0;32m    595\u001b[0m     ckpt_path,\n\u001b[0;32m    596\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    597\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    598\u001b[0m )\n\u001b[1;32m--> 599\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    601\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n",
      "File \u001b[1;32mc:\\Users\\will\\miniconda3\\envs\\Kettering\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1012\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m   1009\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m   1010\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[0;32m   1011\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m-> 1012\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1014\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m   1015\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\will\\miniconda3\\envs\\Kettering\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1056\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1055\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[1;32m-> 1056\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1057\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\will\\miniconda3\\envs\\Kettering\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:216\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[1;32m--> 216\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n",
      "File \u001b[1;32mc:\\Users\\will\\miniconda3\\envs\\Kettering\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:455\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    454\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 455\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\will\\miniconda3\\envs\\Kettering\\Lib\\site-packages\\pytorch_lightning\\loops\\training_epoch_loop.py:150\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[1;34m(self, data_fetcher)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end(data_fetcher)\n",
      "File \u001b[1;32mc:\\Users\\will\\miniconda3\\envs\\Kettering\\Lib\\site-packages\\pytorch_lightning\\loops\\training_epoch_loop.py:320\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[1;34m(self, data_fetcher)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mautomatic_optimization:\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;66;03m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[1;32m--> 320\u001b[0m     batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautomatic_optimization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\will\\miniconda3\\envs\\Kettering\\Lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:192\u001b[0m, in \u001b[0;36m_AutomaticOptimization.run\u001b[1;34m(self, optimizer, batch_idx, kwargs)\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;66;03m# BACKWARD PASS\u001b[39;00m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;66;03m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 192\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    194\u001b[0m result \u001b[38;5;241m=\u001b[39m closure\u001b[38;5;241m.\u001b[39mconsume_result()\n",
      "File \u001b[1;32mc:\\Users\\will\\miniconda3\\envs\\Kettering\\Lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:270\u001b[0m, in \u001b[0;36m_AutomaticOptimization._optimizer_step\u001b[1;34m(self, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;66;03m# model hook\u001b[39;00m\n\u001b[1;32m--> 270\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moptimizer_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    276\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_step_and_backward_closure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    277\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_accumulate:\n",
      "File \u001b[1;32mc:\\Users\\will\\miniconda3\\envs\\Kettering\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:176\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[1;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 176\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\will\\miniconda3\\envs\\Kettering\\Lib\\site-packages\\pytorch_lightning\\core\\module.py:1302\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[1;34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[0m\n\u001b[0;32m   1278\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Override this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[0;32m   1279\u001b[0m \u001b[38;5;124;03mthe optimizer.\u001b[39;00m\n\u001b[0;32m   1280\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1300\u001b[0m \n\u001b[0;32m   1301\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1302\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_closure\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\will\\miniconda3\\envs\\Kettering\\Lib\\site-packages\\pytorch_lightning\\core\\optimizer.py:154\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[1;34m(self, closure, **kwargs)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strategy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 154\u001b[0m step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_after_step()\n",
      "File \u001b[1;32mc:\\Users\\will\\miniconda3\\envs\\Kettering\\Lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:239\u001b[0m, in \u001b[0;36mStrategy.optimizer_step\u001b[1;34m(self, optimizer, closure, model, **kwargs)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, pl\u001b[38;5;241m.\u001b[39mLightningModule)\n\u001b[1;32m--> 239\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecision_plugin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\will\\miniconda3\\envs\\Kettering\\Lib\\site-packages\\pytorch_lightning\\plugins\\precision\\precision.py:123\u001b[0m, in \u001b[0;36mPrecision.optimizer_step\u001b[1;34m(self, optimizer, model, closure, **kwargs)\u001b[0m\n\u001b[0;32m    122\u001b[0m closure \u001b[38;5;241m=\u001b[39m partial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_closure, model, optimizer, closure)\n\u001b[1;32m--> 123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\will\\miniconda3\\envs\\Kettering\\Lib\\site-packages\\torch\\optim\\optimizer.py:485\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    481\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    482\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    483\u001b[0m             )\n\u001b[1;32m--> 485\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    486\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n",
      "File \u001b[1;32mc:\\Users\\will\\miniconda3\\envs\\Kettering\\Lib\\site-packages\\torch\\optim\\optimizer.py:79\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 79\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\will\\miniconda3\\envs\\Kettering\\Lib\\site-packages\\torch\\optim\\adam.py:246\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    236\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    237\u001b[0m         group,\n\u001b[0;32m    238\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    243\u001b[0m         state_steps,\n\u001b[0;32m    244\u001b[0m     )\n\u001b[1;32m--> 246\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    264\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    265\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    266\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    267\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdecoupled_weight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\will\\miniconda3\\envs\\Kettering\\Lib\\site-packages\\torch\\optim\\optimizer.py:147\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\will\\miniconda3\\envs\\Kettering\\Lib\\site-packages\\torch\\optim\\adam.py:933\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    931\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 933\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    934\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    935\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    936\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    937\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    938\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    939\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    940\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    941\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    942\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    943\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    944\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    945\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    947\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    948\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    949\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    950\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    951\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    952\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    953\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\will\\miniconda3\\envs\\Kettering\\Lib\\site-packages\\torch\\optim\\adam.py:525\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[0m\n\u001b[0;32m    524\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 525\u001b[0m     denom \u001b[38;5;241m=\u001b[39m (\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m    527\u001b[0m param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[65], line 42\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Reset the trainer (should be unneeded now) to allow for further training\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# mod.resetTrainer()\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Actually train\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dataSets[i] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 42\u001b[0m     \u001b[43mmod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataSets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;66;03m# combine models\u001b[39;00m\n\u001b[0;32m     44\u001b[0m     nextModel \u001b[38;5;241m=\u001b[39m OBU(nextModel \u001b[38;5;241m+\u001b[39m mod)   \n",
      "Cell \u001b[1;32mIn[64], line 51\u001b[0m, in \u001b[0;36mOBU.fit\u001b[1;34m(self, dataLoader)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataLoader):\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;66;03m# calling built in fit function\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearner\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataLoader\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\will\\miniconda3\\envs\\Kettering\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:561\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    559\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    560\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshould_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 561\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    562\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[0;32m    563\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\will\\miniconda3\\envs\\Kettering\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:65\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(launcher, _SubprocessScriptLauncher):\n\u001b[0;32m     64\u001b[0m         launcher\u001b[38;5;241m.\u001b[39mkill(_get_sigkill_signal())\n\u001b[1;32m---> 65\u001b[0m     \u001b[43mexit\u001b[49m(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[0;32m     68\u001b[0m     _interrupt(trainer, exception)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "# loop through split, and create model per vehicle.\n",
    "# Run one epoch through each model, and average model and return.\n",
    "# Continue loop for desired epochs\n",
    "\n",
    "# With 2 epochs, 10 sub, and 0:3 models ending acc. of 0.1377%\n",
    "# With 2 epochs, 10 sub, and 0:10 models ending acc. of\n",
    "models = []\n",
    "dataSets = []\n",
    "results = []\n",
    "subEpochs = 10\n",
    "epochs = 2\n",
    "gpu = False\n",
    "# Create starting models\n",
    "mainModel = OBU(9, epochs= subEpochs, gpu = gpu)\n",
    "nextModel = OBU(9, epochs= subEpochs, gpu = gpu)\n",
    "# Divide dataset of recieving vehicles among OBUs\n",
    "for vehicle in splits[:50]:\n",
    "    # Add new OBU for each model\n",
    "    models.append(OBU(9, epochs = subEpochs, gpu=gpu))\n",
    "    # Create Slice of dataset\n",
    "    if vehicle.shape[0] < 32:\n",
    "        print(vehicle.shape[0])\n",
    "        vehicle = None\n",
    "    else:\n",
    "        vehicle = data.DataLoader(data.TensorDataset(vehicle[:,:,1:10].float(), vehicle[:,:,11].long()), batch_size=32, shuffle=False, num_workers=16, persistent_workers = True)\n",
    "    # Add sub - dataset to dataset\n",
    "    dataSets.append(vehicle)\n",
    "# Train individual models and combine\n",
    "for epoch in range(epochs):\n",
    "    i = 0\n",
    "    # Baseline model to add everything to. !!Do I want this or should it be a completely new model?!! Got 0% on combination before, testing with new model for next model.\n",
    "    nextModel = OBU(9, epochs= subEpochs, gpu = gpu)\n",
    "    # Train models\n",
    "    for mod in models:\n",
    "        # Make multithreaded?\n",
    "        # set model to main model, and train that\n",
    "        mod = OBU(mainModel)\n",
    "        # Reset the trainer (should be unneeded now) to allow for further training\n",
    "        # mod.resetTrainer()\n",
    "        # Actually train\n",
    "        if dataSets[i] != None:\n",
    "            mod.fit(dataSets[i])\n",
    "            # combine models\n",
    "            nextModel = OBU(nextModel + mod)   \n",
    "            _, _ , perc, _ = mod.test(inTest, outTest)\n",
    "            # Test individual model\n",
    "            results.append([epoch, i+1, perc])\n",
    "            i+=1 \n",
    "        else:\n",
    "            print(\"SKIPPED {epoch}\\n\\n\\n\\n\\n\\n\")\n",
    "    # Create combined model\n",
    "    \n",
    "    mainModel = OBU(nextModel / i)\n",
    "# Test combined model at end\n",
    "_, _, perc, _ = mainModel.test(inTest, outTest)\n",
    "results.append([-1, -1, perc])\n",
    "print(results, file = open('results.txt', 'w'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3f300448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "tensor([[-0.9981, -1.0000,  1.0000, -0.8293, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "          0.6075,  0.0442,  0.7163,  0.6767,  0.7933,  0.7151,  0.5629,  0.9533,\n",
      "         -0.8753,  0.7856, -0.5999, -0.2247],\n",
      "        [-0.9155, -1.0000,  1.0000, -0.7490, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "          0.5863,  0.0659,  0.7266,  0.6708,  0.7904,  0.7140,  0.5576,  0.9531,\n",
      "         -0.8754,  0.7839, -0.6045, -0.2218],\n",
      "        [-0.9489, -1.0000,  1.0000, -0.8079, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "          0.6044,  0.0083,  0.7259,  0.6747,  0.8049,  0.7217,  0.5656,  0.9527,\n",
      "         -0.8770,  0.7812, -0.5921, -0.2208],\n",
      "        [-0.9244, -1.0000,  1.0000, -0.8202, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "          0.5903,  0.0450,  0.7244,  0.6746,  0.7943,  0.7130,  0.5605,  0.9540,\n",
      "         -0.8758,  0.7817, -0.6040, -0.2236],\n",
      "        [-0.8204, -1.0000,  1.0000, -0.8306, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "          0.5760,  0.0688,  0.7257,  0.6738,  0.7918,  0.7142,  0.5560,  0.9537,\n",
      "         -0.8759,  0.7806, -0.6065, -0.2197],\n",
      "        [-0.9052, -1.0000,  1.0000, -0.8287, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "          0.6007,  0.0619,  0.7186,  0.6713,  0.7910,  0.7146,  0.5613,  0.9531,\n",
      "         -0.8752,  0.7856, -0.6016, -0.2228],\n",
      "        [-0.9171, -1.0000,  1.0000, -0.7736, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "          0.5861,  0.0592,  0.7295,  0.6732,  0.7919,  0.7156,  0.5582,  0.9522,\n",
      "         -0.8768,  0.7824, -0.6066, -0.2217],\n",
      "        [-0.9011, -1.0000,  1.0000, -0.8475, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "          0.5786,  0.0812,  0.7179,  0.6703,  0.7889,  0.7076,  0.5549,  0.9542,\n",
      "         -0.8750,  0.7827, -0.6045, -0.2219],\n",
      "        [-0.8631, -1.0000,  1.0000, -0.7902, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "          0.5965,  0.0589,  0.7250,  0.6712,  0.7918,  0.7152,  0.5585,  0.9532,\n",
      "         -0.8754,  0.7846, -0.6021, -0.2216],\n",
      "        [-0.9376, -1.0000,  1.0000, -0.8336, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "          0.6009,  0.0534,  0.7154,  0.6759,  0.7930,  0.7149,  0.5618,  0.9533,\n",
      "         -0.8752,  0.7852, -0.6004, -0.2241],\n",
      "        [-0.9918, -1.0000,  1.0000, -0.7738, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "          0.6079,  0.0590,  0.7221,  0.6598,  0.7916,  0.7140,  0.5577,  0.9529,\n",
      "         -0.8750,  0.7859, -0.6021, -0.2236],\n",
      "        [-0.9324, -1.0000,  1.0000, -0.8054, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "          0.6019,  0.0490,  0.7242,  0.6739,  0.7940,  0.7152,  0.5615,  0.9533,\n",
      "         -0.8758,  0.7848, -0.6005, -0.2245],\n",
      "        [-0.9729, -1.0000,  1.0000, -0.8740, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "          0.6004,  0.0527,  0.7156,  0.6718,  0.7918,  0.7144,  0.5609,  0.9533,\n",
      "         -0.8754,  0.7845, -0.6022, -0.2225],\n",
      "        [-0.9392, -1.0000,  1.0000, -0.7578, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "          0.6012,  0.0588,  0.7272,  0.6672,  0.7925,  0.7125,  0.5599,  0.9530,\n",
      "         -0.8754,  0.7839, -0.6021, -0.2222],\n",
      "        [-0.9753, -1.0000,  1.0000, -0.8987, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "          0.6006,  0.0513,  0.7112,  0.6751,  0.7920,  0.7149,  0.5619,  0.9533,\n",
      "         -0.8752,  0.7849, -0.6016, -0.2230],\n",
      "        [-0.9694, -1.0000,  1.0000, -0.8849, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "          0.6046,  0.0526,  0.7158,  0.6742,  0.7929,  0.7148,  0.5624,  0.9533,\n",
      "         -0.8754,  0.7853, -0.6007, -0.2234],\n",
      "        [-0.9733, -1.0000,  1.0000, -0.8345, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "          0.6086,  0.0414,  0.7199,  0.6723,  0.7945,  0.7157,  0.5638,  0.9532,\n",
      "         -0.8756,  0.7850, -0.5993, -0.2237],\n",
      "        [-0.9559, -1.0000,  1.0000, -0.8139, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "          0.6064,  0.0446,  0.7236,  0.6764,  0.7946,  0.7156,  0.5635,  0.9533,\n",
      "         -0.8757,  0.7853, -0.5992, -0.2243],\n",
      "        [-0.9895, -1.0000,  1.0000, -0.8400, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "          0.6053,  0.0515,  0.7152,  0.6766,  0.7923,  0.7138,  0.5629,  0.9533,\n",
      "         -0.8753,  0.7853, -0.6006, -0.2246],\n",
      "        [-0.9850, -1.0000,  1.0000, -0.8246, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "          0.6057,  0.0509,  0.7189,  0.6756,  0.7923,  0.7139,  0.5636,  0.9530,\n",
      "         -0.8761,  0.7846, -0.6011, -0.2232],\n",
      "        [-0.9141, -1.0000,  1.0000, -0.8038, -1.0000, -1.0000,  0.9986, -1.0000,\n",
      "          0.5877,  0.0757,  0.7221,  0.6766,  0.7851,  0.7114,  0.5568,  0.9535,\n",
      "         -0.8753,  0.7852, -0.6084, -0.2252],\n",
      "        [-1.0000, -1.0000,  1.0000, -0.7906, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "          0.6010,  0.0500,  0.7189,  0.6741,  0.7931,  0.7138,  0.5617,  0.9541,\n",
      "         -0.8750,  0.7851, -0.6002, -0.2256],\n",
      "        [-0.9512, -1.0000,  1.0000, -0.7711, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "          0.5806,  0.0660,  0.7224,  0.6653,  0.7907,  0.7124,  0.5555,  0.9528,\n",
      "         -0.8753,  0.7823, -0.6020, -0.2221],\n",
      "        [-0.9773, -1.0000,  1.0000, -0.6992, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "          0.5976,  0.0353,  0.7326,  0.6754,  0.7977,  0.7173,  0.5616,  0.9533,\n",
      "         -0.8767,  0.7812, -0.5994, -0.2206],\n",
      "        [-0.8841, -1.0000,  1.0000, -0.7073, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "          0.5803,  0.0804,  0.7310,  0.6690,  0.7865,  0.7111,  0.5568,  0.9522,\n",
      "         -0.8763,  0.7848, -0.6070, -0.2240],\n",
      "        [-0.9066, -1.0000,  1.0000, -0.8235, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "          0.5815,  0.0613,  0.7235,  0.6754,  0.7908,  0.7140,  0.5575,  0.9535,\n",
      "         -0.8758,  0.7806, -0.6078, -0.2208],\n",
      "        [-0.9674, -1.0000,  1.0000, -0.7686, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "          0.6023,  0.0395,  0.7230,  0.6756,  0.7967,  0.7166,  0.5635,  0.9533,\n",
      "         -0.8754,  0.7849, -0.5974, -0.2249],\n",
      "        [-1.0000, -1.0000,  1.0000, -0.8173, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "          0.6059,  0.0477,  0.7172,  0.6753,  0.7931,  0.7147,  0.5630,  0.9534,\n",
      "         -0.8753,  0.7853, -0.6001, -0.2245],\n",
      "        [-0.9537, -1.0000,  1.0000, -0.8257, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "          0.5943,  0.0285,  0.7231,  0.6745,  0.7992,  0.7157,  0.5622,  0.9538,\n",
      "         -0.8760,  0.7806, -0.5997, -0.2230],\n",
      "        [-0.9737, -1.0000,  1.0000, -0.7748, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "          0.6039,  0.0499,  0.7228,  0.6780,  0.7933,  0.7130,  0.5619,  0.9543,\n",
      "         -0.8753,  0.7851, -0.6001, -0.2259],\n",
      "        [-0.9677, -1.0000,  1.0000, -0.7541, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "          0.5983,  0.0546,  0.7248,  0.6760,  0.7928,  0.7140,  0.5608,  0.9534,\n",
      "         -0.8756,  0.7852, -0.6007, -0.2250],\n",
      "        [-0.9449, -1.0000,  1.0000, -0.8400, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "          0.5912,  0.0640,  0.7158,  0.6795,  0.7895,  0.7091,  0.5596,  0.9541,\n",
      "         -0.8751,  0.7837, -0.6026, -0.2254]])\n",
      "tensor([[-0.9981, -1.0000,  1.0000, -0.8293, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "          0.6075,  0.0442,  0.7163,  0.6767,  0.7933,  0.7151,  0.5629,  0.9533,\n",
      "         -0.8753,  0.7856, -0.5999, -0.2247],\n",
      "        [-0.9155, -1.0000,  1.0000, -0.7490, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "          0.5863,  0.0659,  0.7266,  0.6708,  0.7904,  0.7140,  0.5576,  0.9531,\n",
      "         -0.8754,  0.7839, -0.6045, -0.2218],\n",
      "        [-0.9489, -1.0000,  1.0000, -0.8079, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "          0.6044,  0.0083,  0.7259,  0.6747,  0.8049,  0.7217,  0.5656,  0.9527,\n",
      "         -0.8770,  0.7812, -0.5921, -0.2208],\n",
      "        [-0.9244, -1.0000,  1.0000, -0.8202, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "          0.5903,  0.0450,  0.7244,  0.6746,  0.7943,  0.7130,  0.5605,  0.9540,\n",
      "         -0.8758,  0.7817, -0.6040, -0.2236],\n",
      "        [-0.8204, -1.0000,  1.0000, -0.8306, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "          0.5760,  0.0688,  0.7257,  0.6738,  0.7918,  0.7142,  0.5560,  0.9537,\n",
      "         -0.8759,  0.7806, -0.6065, -0.2197],\n",
      "        [-0.9052, -1.0000,  1.0000, -0.8287, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "          0.6007,  0.0619,  0.7186,  0.6713,  0.7910,  0.7146,  0.5613,  0.9531,\n",
      "         -0.8752,  0.7856, -0.6016, -0.2228],\n",
      "        [-0.9171, -1.0000,  1.0000, -0.7736, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "          0.5861,  0.0592,  0.7295,  0.6732,  0.7919,  0.7156,  0.5582,  0.9522,\n",
      "         -0.8768,  0.7824, -0.6066, -0.2217],\n",
      "        [-0.9011, -1.0000,  1.0000, -0.8475, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "          0.5786,  0.0812,  0.7179,  0.6703,  0.7889,  0.7076,  0.5549,  0.9542,\n",
      "         -0.8750,  0.7827, -0.6045, -0.2219],\n",
      "        [-0.8631, -1.0000,  1.0000, -0.7902, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "          0.5965,  0.0589,  0.7250,  0.6712,  0.7918,  0.7152,  0.5585,  0.9532,\n",
      "         -0.8754,  0.7846, -0.6021, -0.2216],\n",
      "        [-0.9376, -1.0000,  1.0000, -0.8336, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "          0.6009,  0.0534,  0.7154,  0.6759,  0.7930,  0.7149,  0.5618,  0.9533,\n",
      "         -0.8752,  0.7852, -0.6004, -0.2241],\n",
      "        [-0.9918, -1.0000,  1.0000, -0.7738, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "          0.6079,  0.0590,  0.7221,  0.6598,  0.7916,  0.7140,  0.5577,  0.9529,\n",
      "         -0.8750,  0.7859, -0.6021, -0.2236],\n",
      "        [-0.9324, -1.0000,  1.0000, -0.8054, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "          0.6019,  0.0490,  0.7242,  0.6739,  0.7940,  0.7152,  0.5615,  0.9533,\n",
      "         -0.8758,  0.7848, -0.6005, -0.2245],\n",
      "        [-0.9729, -1.0000,  1.0000, -0.8740, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "          0.6004,  0.0527,  0.7156,  0.6718,  0.7918,  0.7144,  0.5609,  0.9533,\n",
      "         -0.8754,  0.7845, -0.6022, -0.2225],\n",
      "        [-0.9392, -1.0000,  1.0000, -0.7578, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "          0.6012,  0.0588,  0.7272,  0.6672,  0.7925,  0.7125,  0.5599,  0.9530,\n",
      "         -0.8754,  0.7839, -0.6021, -0.2222],\n",
      "        [-0.9753, -1.0000,  1.0000, -0.8987, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "          0.6006,  0.0513,  0.7112,  0.6751,  0.7920,  0.7149,  0.5619,  0.9533,\n",
      "         -0.8752,  0.7849, -0.6016, -0.2230],\n",
      "        [-0.9694, -1.0000,  1.0000, -0.8849, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "          0.6046,  0.0526,  0.7158,  0.6742,  0.7929,  0.7148,  0.5624,  0.9533,\n",
      "         -0.8754,  0.7853, -0.6007, -0.2234],\n",
      "        [-0.9733, -1.0000,  1.0000, -0.8345, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "          0.6086,  0.0414,  0.7199,  0.6723,  0.7945,  0.7157,  0.5638,  0.9532,\n",
      "         -0.8756,  0.7850, -0.5993, -0.2237],\n",
      "        [-0.9559, -1.0000,  1.0000, -0.8139, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "          0.6064,  0.0446,  0.7236,  0.6764,  0.7946,  0.7156,  0.5635,  0.9533,\n",
      "         -0.8757,  0.7853, -0.5992, -0.2243],\n",
      "        [-0.9895, -1.0000,  1.0000, -0.8400, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "          0.6053,  0.0515,  0.7152,  0.6766,  0.7923,  0.7138,  0.5629,  0.9533,\n",
      "         -0.8753,  0.7853, -0.6006, -0.2246],\n",
      "        [-0.9850, -1.0000,  1.0000, -0.8246, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "          0.6057,  0.0509,  0.7189,  0.6756,  0.7923,  0.7139,  0.5636,  0.9530,\n",
      "         -0.8761,  0.7846, -0.6011, -0.2232],\n",
      "        [-0.9141, -1.0000,  1.0000, -0.8038, -1.0000, -1.0000,  0.9986, -1.0000,\n",
      "          0.5877,  0.0757,  0.7221,  0.6766,  0.7851,  0.7114,  0.5568,  0.9535,\n",
      "         -0.8753,  0.7852, -0.6084, -0.2252],\n",
      "        [-1.0000, -1.0000,  1.0000, -0.7906, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "          0.6010,  0.0500,  0.7189,  0.6741,  0.7931,  0.7138,  0.5617,  0.9541,\n",
      "         -0.8750,  0.7851, -0.6002, -0.2256],\n",
      "        [-0.9512, -1.0000,  1.0000, -0.7711, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "          0.5806,  0.0660,  0.7224,  0.6653,  0.7907,  0.7124,  0.5555,  0.9528,\n",
      "         -0.8753,  0.7823, -0.6020, -0.2221],\n",
      "        [-0.9773, -1.0000,  1.0000, -0.6992, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "          0.5976,  0.0353,  0.7326,  0.6754,  0.7977,  0.7173,  0.5616,  0.9533,\n",
      "         -0.8767,  0.7812, -0.5994, -0.2206],\n",
      "        [-0.8841, -1.0000,  1.0000, -0.7073, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "          0.5803,  0.0804,  0.7310,  0.6690,  0.7865,  0.7111,  0.5568,  0.9522,\n",
      "         -0.8763,  0.7848, -0.6070, -0.2240],\n",
      "        [-0.9066, -1.0000,  1.0000, -0.8235, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "          0.5815,  0.0613,  0.7235,  0.6754,  0.7908,  0.7140,  0.5575,  0.9535,\n",
      "         -0.8758,  0.7806, -0.6078, -0.2208],\n",
      "        [-0.9674, -1.0000,  1.0000, -0.7686, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "          0.6023,  0.0395,  0.7230,  0.6756,  0.7967,  0.7166,  0.5635,  0.9533,\n",
      "         -0.8754,  0.7849, -0.5974, -0.2249],\n",
      "        [-1.0000, -1.0000,  1.0000, -0.8173, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "          0.6059,  0.0477,  0.7172,  0.6753,  0.7931,  0.7147,  0.5630,  0.9534,\n",
      "         -0.8753,  0.7853, -0.6001, -0.2245],\n",
      "        [-0.9537, -1.0000,  1.0000, -0.8257, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "          0.5943,  0.0285,  0.7231,  0.6745,  0.7992,  0.7157,  0.5622,  0.9538,\n",
      "         -0.8760,  0.7806, -0.5997, -0.2230],\n",
      "        [-0.9737, -1.0000,  1.0000, -0.7748, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "          0.6039,  0.0499,  0.7228,  0.6780,  0.7933,  0.7130,  0.5619,  0.9543,\n",
      "         -0.8753,  0.7851, -0.6001, -0.2259],\n",
      "        [-0.9677, -1.0000,  1.0000, -0.7541, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "          0.5983,  0.0546,  0.7248,  0.6760,  0.7928,  0.7140,  0.5608,  0.9534,\n",
      "         -0.8756,  0.7852, -0.6007, -0.2250],\n",
      "        [-0.9449, -1.0000,  1.0000, -0.8400, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "          0.5912,  0.0640,  0.7158,  0.6795,  0.7895,  0.7091,  0.5596,  0.9541,\n",
      "         -0.8751,  0.7837, -0.6026, -0.2254]])\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "1\n",
      "tensor([[-7.6004e-01,  1.0000e+00, -5.3941e-02,  1.2167e-01, -1.0000e+00,\n",
      "         -1.0000e+00,  9.9998e-01,  1.0000e+00,  6.8931e-01, -6.0010e-01,\n",
      "          6.2882e-01, -3.5610e-03,  6.8400e-01, -2.3380e-01,  8.4522e-01,\n",
      "          7.1126e-01, -8.9890e-01,  8.0388e-01, -7.1784e-01,  6.1549e-01],\n",
      "        [-5.2351e-01,  1.0000e+00, -6.7182e-02,  2.8477e-01, -1.0000e+00,\n",
      "         -1.0000e+00,  9.9999e-01,  1.0000e+00,  6.2850e-01, -5.5974e-01,\n",
      "          6.1508e-01, -2.2991e-02,  6.8370e-01, -2.2596e-01,  8.4608e-01,\n",
      "          7.0229e-01, -8.9671e-01,  7.8804e-01, -6.9764e-01,  6.0886e-01],\n",
      "        [-2.9397e-01,  1.0000e+00, -2.8543e-02,  1.2000e-01, -1.0000e+00,\n",
      "         -1.0000e+00,  9.9998e-01,  1.0000e+00,  5.0971e-01, -4.8665e-01,\n",
      "          6.5966e-01,  3.7812e-02,  6.7772e-01, -1.9162e-01,  8.5490e-01,\n",
      "          6.9819e-01, -8.9955e-01,  7.6936e-01, -6.5779e-01,  6.0142e-01],\n",
      "        [-3.9620e-01,  1.0000e+00, -5.8512e-02,  1.2215e-01, -1.0000e+00,\n",
      "         -1.0000e+00,  9.9998e-01,  1.0000e+00,  5.1087e-01, -4.9978e-01,\n",
      "          6.7652e-01,  5.1374e-02,  6.7809e-01, -1.8451e-01,  8.6169e-01,\n",
      "          7.0624e-01, -9.0138e-01,  7.8450e-01, -6.7374e-01,  6.0716e-01],\n",
      "        [-2.9606e-01,  1.0000e+00, -8.7040e-02,  1.2809e-01, -1.0000e+00,\n",
      "         -1.0000e+00,  1.0000e+00,  1.0000e+00,  4.8698e-01, -4.5486e-01,\n",
      "          6.8331e-01,  7.2306e-02,  6.7520e-01, -1.7345e-01,  8.5784e-01,\n",
      "          7.0088e-01, -9.0219e-01,  7.6329e-01, -6.3902e-01,  5.9577e-01],\n",
      "        [-4.4712e-01,  1.0000e+00, -1.1246e-01,  1.2019e-01, -1.0000e+00,\n",
      "         -1.0000e+00,  1.0000e+00,  1.0000e+00,  6.0078e-01, -4.8815e-01,\n",
      "          6.4848e-01,  2.2428e-03,  6.8116e-01, -1.9600e-01,  8.5029e-01,\n",
      "          7.0572e-01, -9.0013e-01,  7.7411e-01, -6.6896e-01,  6.0602e-01],\n",
      "        [-5.2847e-01,  1.0000e+00, -8.0144e-02, -2.0762e-01, -1.0000e+00,\n",
      "         -1.0000e+00,  9.5270e-01,  1.0000e+00,  5.5962e-01, -5.2323e-01,\n",
      "          6.8406e-01,  5.5843e-02,  6.8117e-01, -1.9760e-01,  8.5694e-01,\n",
      "          7.0352e-01, -9.0436e-01,  7.7430e-01, -6.6449e-01,  5.9954e-01],\n",
      "        [-2.6348e-01,  1.0000e+00,  8.1390e-04,  6.0000e-01, -1.0000e+00,\n",
      "         -1.0000e+00,  9.4145e-01,  1.0000e+00,  6.0004e-01, -6.2866e-01,\n",
      "          5.7918e-01,  6.1046e-02,  6.7698e-01, -2.3972e-01,  8.4279e-01,\n",
      "          6.8307e-01, -8.9095e-01,  8.2811e-01, -7.1784e-01,  6.1567e-01],\n",
      "        [-6.7680e-02,  1.0000e+00, -5.0437e-02,  3.6782e-01, -1.0000e+00,\n",
      "         -1.0000e+00,  9.4083e-01,  1.0000e+00,  5.3670e-01, -4.6674e-01,\n",
      "          6.2202e-01,  5.6494e-02,  6.7147e-01, -1.9982e-01,  8.5170e-01,\n",
      "          6.8838e-01, -8.9936e-01,  7.8231e-01, -6.5704e-01,  5.9941e-01],\n",
      "        [-4.3367e-01,  1.0000e+00, -8.9936e-02,  2.7977e-01, -1.0000e+00,\n",
      "         -1.0000e+00,  9.5403e-01,  1.0000e+00,  5.7070e-01, -4.7158e-01,\n",
      "          6.5662e-01,  2.8846e-02,  6.7704e-01, -1.8991e-01,  8.4506e-01,\n",
      "          6.9813e-01, -8.9968e-01,  7.7186e-01, -6.5605e-01,  5.9731e-01],\n",
      "        [-6.0301e-01,  1.0000e+00, -7.2631e-02,  3.6000e-01, -1.0000e+00,\n",
      "         -1.0000e+00,  9.5549e-01,  1.0000e+00,  5.6927e-01, -5.7504e-01,\n",
      "          6.4534e-01,  1.0992e-02,  6.8178e-01, -2.2210e-01,  8.4829e-01,\n",
      "          7.0010e-01, -8.9739e-01,  7.9001e-01, -6.8685e-01,  6.0153e-01],\n",
      "        [-4.3930e-01,  1.0000e+00, -8.5803e-02,  7.1208e-02, -1.0000e+00,\n",
      "         -1.0000e+00,  9.8491e-01,  1.0000e+00,  5.5791e-01, -5.4432e-01,\n",
      "          6.3929e-01,  3.9419e-02,  6.7974e-01, -2.3477e-01,  8.5123e-01,\n",
      "          6.9970e-01, -9.0100e-01,  7.8538e-01, -6.9074e-01,  6.0372e-01],\n",
      "        [-5.9979e-01,  1.0000e+00, -6.1506e-02,  3.7745e-01, -1.0000e+00,\n",
      "         -1.0000e+00,  9.9203e-01,  1.0000e+00,  6.6425e-01, -5.9383e-01,\n",
      "          5.8181e-01, -8.0496e-03,  6.8162e-01, -2.5719e-01,  8.4089e-01,\n",
      "          6.9056e-01, -8.9271e-01,  7.9174e-01, -7.2459e-01,  6.1102e-01],\n",
      "        [-4.4423e-01,  1.0000e+00, -4.7259e-02,  2.8673e-01, -1.0000e+00,\n",
      "         -1.0000e+00,  9.8531e-01,  1.0000e+00,  5.8550e-01, -5.8249e-01,\n",
      "          6.1849e-01,  4.4572e-02,  6.8070e-01, -2.3022e-01,  8.4594e-01,\n",
      "          6.9139e-01, -8.9603e-01,  7.9950e-01, -6.9791e-01,  6.0351e-01],\n",
      "        [-7.5322e-01,  1.0000e+00, -7.3768e-02,  2.1194e-01, -1.0000e+00,\n",
      "         -1.0000e+00,  9.8778e-01,  1.0000e+00,  6.3902e-01, -5.5622e-01,\n",
      "          6.3523e-01,  8.4427e-03,  6.8582e-01, -2.1038e-01,  8.3593e-01,\n",
      "          6.9710e-01, -8.9619e-01,  7.9240e-01, -6.8889e-01,  6.0078e-01],\n",
      "        [-4.9345e-01,  1.0000e+00,  6.5766e-03,  5.1921e-01, -1.0000e+00,\n",
      "         -1.0000e+00,  9.8768e-01,  1.0000e+00,  5.8698e-01, -5.2698e-01,\n",
      "          6.0503e-01,  2.9506e-03,  6.8361e-01, -2.1062e-01,  8.4060e-01,\n",
      "          6.8893e-01, -8.9588e-01,  7.8154e-01, -6.7393e-01,  5.9950e-01],\n",
      "        [-6.6881e-01,  1.0000e+00, -3.1412e-02,  4.4024e-01, -1.0000e+00,\n",
      "         -1.0000e+00,  9.9966e-01,  1.0000e+00,  6.1310e-01, -5.4831e-01,\n",
      "          6.2645e-01, -4.7085e-02,  6.8572e-01, -2.1408e-01,  8.4949e-01,\n",
      "          6.9958e-01, -8.9808e-01,  7.8380e-01, -7.1372e-01,  6.0635e-01],\n",
      "        [-4.1909e-01,  1.0000e+00, -5.1049e-02,  2.0179e-01, -1.0000e+00,\n",
      "         -1.0000e+00,  9.9965e-01,  1.0000e+00,  5.8603e-01, -6.2594e-01,\n",
      "          6.0999e-01,  1.7881e-02,  6.8255e-01, -2.6375e-01,  8.5572e-01,\n",
      "          7.0567e-01, -8.9712e-01,  8.1128e-01, -7.2798e-01,  6.2107e-01],\n",
      "        [-5.9088e-01,  1.0000e+00, -5.5411e-02,  4.0000e-02, -1.0000e+00,\n",
      "         -1.0000e+00,  9.9903e-01,  1.0000e+00,  6.5655e-01, -4.9866e-01,\n",
      "          6.3798e-01, -5.6846e-02,  6.8560e-01, -1.9666e-01,  8.4262e-01,\n",
      "          7.1566e-01, -8.9689e-01,  7.6546e-01, -6.8881e-01,  6.1189e-01],\n",
      "        [-5.4756e-01,  1.0000e+00, -6.1967e-02,  2.1020e-01, -1.0000e+00,\n",
      "         -1.0000e+00,  9.9875e-01,  1.0000e+00,  6.2215e-01, -5.8058e-01,\n",
      "          6.2502e-01,  1.6103e-02,  6.8376e-01, -2.2923e-01,  8.5519e-01,\n",
      "          7.0094e-01, -9.0102e-01,  8.0436e-01, -7.0306e-01,  6.1677e-01],\n",
      "        [-5.6992e-01,  1.0000e+00, -2.3950e-02,  1.4307e-01, -1.0000e+00,\n",
      "         -1.0000e+00,  9.6405e-01,  1.0000e+00,  5.6233e-01, -5.8091e-01,\n",
      "          6.3099e-01,  2.6197e-03,  6.8453e-01, -2.2936e-01,  8.5836e-01,\n",
      "          7.0488e-01, -9.0117e-01,  7.9474e-01, -6.9062e-01,  6.1103e-01],\n",
      "        [-5.3009e-01,  1.0000e+00, -3.0723e-02,  2.7771e-01, -1.0000e+00,\n",
      "         -1.0000e+00,  9.5569e-01,  1.0000e+00,  5.7490e-01, -5.5057e-01,\n",
      "          6.0230e-01,  4.9114e-02,  6.7852e-01, -2.1473e-01,  8.5003e-01,\n",
      "          6.9782e-01, -8.9235e-01,  8.0483e-01, -6.7455e-01,  6.0922e-01],\n",
      "        [-6.1798e-01,  1.0000e+00, -5.1503e-02,  3.5995e-01, -1.0000e+00,\n",
      "         -1.0000e+00,  9.9021e-01,  1.0000e+00,  6.6402e-01, -5.6831e-01,\n",
      "          6.1688e-01, -4.9039e-02,  6.7791e-01, -2.4120e-01,  8.3587e-01,\n",
      "          7.0723e-01, -8.9237e-01,  7.8437e-01, -7.1962e-01,  6.0575e-01],\n",
      "        [-3.7844e-01,  1.0000e+00, -2.9048e-02,  2.9428e-01, -1.0000e+00,\n",
      "         -1.0000e+00,  9.9372e-01,  1.0000e+00,  6.1822e-01, -5.0393e-01,\n",
      "          6.4621e-01, -4.1931e-03,  6.7718e-01, -2.0337e-01,  8.5033e-01,\n",
      "          7.0935e-01, -8.9779e-01,  7.8667e-01, -6.9938e-01,  6.1248e-01],\n",
      "        [-4.1601e-01,  1.0000e+00,  4.9452e-02,  1.2000e-01, -1.0000e+00,\n",
      "         -1.0000e+00,  9.9891e-01,  1.0000e+00,  5.9406e-01, -5.7535e-01,\n",
      "          5.8266e-01,  4.3930e-03,  6.7587e-01, -2.5639e-01,  8.5471e-01,\n",
      "          6.9942e-01, -8.9558e-01,  8.0197e-01, -7.1057e-01,  6.1511e-01],\n",
      "        [-6.7404e-01,  1.0000e+00,  4.7630e-02,  3.9309e-02, -1.0000e+00,\n",
      "         -1.0000e+00,  9.8595e-01,  1.0000e+00,  5.8779e-01, -6.3760e-01,\n",
      "          5.9105e-01,  3.1082e-02,  6.8193e-01, -2.6287e-01,  8.5132e-01,\n",
      "          7.0196e-01, -8.9649e-01,  8.0761e-01, -7.0490e-01,  6.0651e-01],\n",
      "        [-4.4523e-01,  1.0000e+00, -1.1603e-02,  2.8132e-01, -1.0000e+00,\n",
      "         -1.0000e+00,  9.9993e-01,  1.0000e+00,  6.2176e-01, -5.7226e-01,\n",
      "          6.1429e-01,  2.8279e-02,  6.8193e-01, -2.1611e-01,  8.4482e-01,\n",
      "          6.9787e-01, -8.9749e-01,  8.0521e-01, -6.9726e-01,  6.1018e-01],\n",
      "        [-6.5224e-01,  1.0000e+00,  4.0281e-02,  4.3995e-01, -1.0000e+00,\n",
      "         -1.0000e+00,  9.9975e-01,  1.0000e+00,  6.6267e-01, -5.1847e-01,\n",
      "          6.0498e-01, -3.2852e-02,  6.7972e-01, -2.0797e-01,  8.3537e-01,\n",
      "          6.9748e-01, -8.9450e-01,  7.9390e-01, -7.0511e-01,  6.0661e-01],\n",
      "        [-5.1558e-01,  1.0000e+00, -5.0434e-02,  2.1661e-01, -1.0000e+00,\n",
      "         -1.0000e+00,  9.8835e-01,  1.0000e+00,  5.7560e-01, -5.6287e-01,\n",
      "          6.7171e-01,  2.1279e-02,  6.8625e-01, -2.0386e-01,  8.5600e-01,\n",
      "          7.0326e-01, -9.0228e-01,  7.8934e-01, -6.8833e-01,  6.1152e-01],\n",
      "        [-6.4174e-01,  1.0000e+00, -7.9828e-02,  2.8032e-01, -1.0000e+00,\n",
      "         -1.0000e+00,  9.9974e-01,  1.0000e+00,  6.6740e-01, -6.2698e-01,\n",
      "          5.9974e-01,  2.2514e-02,  6.8551e-01, -2.3667e-01,  8.3939e-01,\n",
      "          6.9754e-01, -8.9532e-01,  8.1423e-01, -7.1877e-01,  6.1125e-01],\n",
      "        [-5.6957e-01,  1.0000e+00, -6.9399e-02,  2.8000e-01, -1.0000e+00,\n",
      "         -1.0000e+00,  9.9969e-01,  1.0000e+00,  6.4837e-01, -5.3300e-01,\n",
      "          6.1199e-01,  8.0501e-03,  6.8219e-01, -2.2560e-01,  8.4175e-01,\n",
      "          6.9821e-01, -9.0058e-01,  7.8713e-01, -6.8998e-01,  6.0448e-01],\n",
      "        [-5.1998e-01,  1.0000e+00, -6.5907e-02,  3.9899e-01, -1.0000e+00,\n",
      "         -1.0000e+00,  1.0000e+00,  1.0000e+00,  6.4840e-01, -4.8928e-01,\n",
      "          6.2930e-01, -1.5987e-02,  6.7582e-01, -2.0421e-01,  8.3871e-01,\n",
      "          7.0149e-01, -8.9789e-01,  7.7371e-01, -6.8137e-01,  6.0885e-01]])\n",
      "tensor([[-7.6004e-01,  1.0000e+00, -5.3941e-02,  1.2167e-01, -1.0000e+00,\n",
      "         -1.0000e+00,  9.9998e-01,  1.0000e+00,  6.8931e-01, -6.0010e-01,\n",
      "          6.2882e-01, -3.5610e-03,  6.8400e-01, -2.3380e-01,  8.4522e-01,\n",
      "          7.1126e-01, -8.9890e-01,  8.0388e-01, -7.1784e-01,  6.1549e-01],\n",
      "        [-5.2351e-01,  1.0000e+00, -6.7182e-02,  2.8477e-01, -1.0000e+00,\n",
      "         -1.0000e+00,  9.9999e-01,  1.0000e+00,  6.2850e-01, -5.5974e-01,\n",
      "          6.1508e-01, -2.2991e-02,  6.8370e-01, -2.2596e-01,  8.4608e-01,\n",
      "          7.0229e-01, -8.9671e-01,  7.8804e-01, -6.9764e-01,  6.0886e-01],\n",
      "        [-2.9397e-01,  1.0000e+00, -2.8543e-02,  1.2000e-01, -1.0000e+00,\n",
      "         -1.0000e+00,  9.9998e-01,  1.0000e+00,  5.0971e-01, -4.8665e-01,\n",
      "          6.5966e-01,  3.7812e-02,  6.7772e-01, -1.9162e-01,  8.5490e-01,\n",
      "          6.9819e-01, -8.9955e-01,  7.6936e-01, -6.5779e-01,  6.0142e-01],\n",
      "        [-3.9620e-01,  1.0000e+00, -5.8512e-02,  1.2215e-01, -1.0000e+00,\n",
      "         -1.0000e+00,  9.9998e-01,  1.0000e+00,  5.1087e-01, -4.9978e-01,\n",
      "          6.7652e-01,  5.1374e-02,  6.7809e-01, -1.8451e-01,  8.6169e-01,\n",
      "          7.0624e-01, -9.0138e-01,  7.8450e-01, -6.7374e-01,  6.0716e-01],\n",
      "        [-2.9606e-01,  1.0000e+00, -8.7040e-02,  1.2809e-01, -1.0000e+00,\n",
      "         -1.0000e+00,  1.0000e+00,  1.0000e+00,  4.8698e-01, -4.5486e-01,\n",
      "          6.8331e-01,  7.2306e-02,  6.7520e-01, -1.7345e-01,  8.5784e-01,\n",
      "          7.0088e-01, -9.0219e-01,  7.6329e-01, -6.3902e-01,  5.9577e-01],\n",
      "        [-4.4712e-01,  1.0000e+00, -1.1246e-01,  1.2019e-01, -1.0000e+00,\n",
      "         -1.0000e+00,  1.0000e+00,  1.0000e+00,  6.0078e-01, -4.8815e-01,\n",
      "          6.4848e-01,  2.2428e-03,  6.8116e-01, -1.9600e-01,  8.5029e-01,\n",
      "          7.0572e-01, -9.0013e-01,  7.7411e-01, -6.6896e-01,  6.0602e-01],\n",
      "        [-5.2847e-01,  1.0000e+00, -8.0144e-02, -2.0762e-01, -1.0000e+00,\n",
      "         -1.0000e+00,  9.5270e-01,  1.0000e+00,  5.5962e-01, -5.2323e-01,\n",
      "          6.8406e-01,  5.5843e-02,  6.8117e-01, -1.9760e-01,  8.5694e-01,\n",
      "          7.0352e-01, -9.0436e-01,  7.7430e-01, -6.6449e-01,  5.9954e-01],\n",
      "        [-2.6348e-01,  1.0000e+00,  8.1390e-04,  6.0000e-01, -1.0000e+00,\n",
      "         -1.0000e+00,  9.4145e-01,  1.0000e+00,  6.0004e-01, -6.2866e-01,\n",
      "          5.7918e-01,  6.1046e-02,  6.7698e-01, -2.3972e-01,  8.4279e-01,\n",
      "          6.8307e-01, -8.9095e-01,  8.2811e-01, -7.1784e-01,  6.1567e-01],\n",
      "        [-6.7680e-02,  1.0000e+00, -5.0437e-02,  3.6782e-01, -1.0000e+00,\n",
      "         -1.0000e+00,  9.4083e-01,  1.0000e+00,  5.3670e-01, -4.6674e-01,\n",
      "          6.2202e-01,  5.6494e-02,  6.7147e-01, -1.9982e-01,  8.5170e-01,\n",
      "          6.8838e-01, -8.9936e-01,  7.8231e-01, -6.5704e-01,  5.9941e-01],\n",
      "        [-4.3367e-01,  1.0000e+00, -8.9936e-02,  2.7977e-01, -1.0000e+00,\n",
      "         -1.0000e+00,  9.5403e-01,  1.0000e+00,  5.7070e-01, -4.7158e-01,\n",
      "          6.5662e-01,  2.8846e-02,  6.7704e-01, -1.8991e-01,  8.4506e-01,\n",
      "          6.9813e-01, -8.9968e-01,  7.7186e-01, -6.5605e-01,  5.9731e-01],\n",
      "        [-6.0301e-01,  1.0000e+00, -7.2631e-02,  3.6000e-01, -1.0000e+00,\n",
      "         -1.0000e+00,  9.5549e-01,  1.0000e+00,  5.6927e-01, -5.7504e-01,\n",
      "          6.4534e-01,  1.0992e-02,  6.8178e-01, -2.2210e-01,  8.4829e-01,\n",
      "          7.0010e-01, -8.9739e-01,  7.9001e-01, -6.8685e-01,  6.0153e-01],\n",
      "        [-4.3930e-01,  1.0000e+00, -8.5803e-02,  7.1208e-02, -1.0000e+00,\n",
      "         -1.0000e+00,  9.8491e-01,  1.0000e+00,  5.5791e-01, -5.4432e-01,\n",
      "          6.3929e-01,  3.9419e-02,  6.7974e-01, -2.3477e-01,  8.5123e-01,\n",
      "          6.9970e-01, -9.0100e-01,  7.8538e-01, -6.9074e-01,  6.0372e-01],\n",
      "        [-5.9979e-01,  1.0000e+00, -6.1506e-02,  3.7745e-01, -1.0000e+00,\n",
      "         -1.0000e+00,  9.9203e-01,  1.0000e+00,  6.6425e-01, -5.9383e-01,\n",
      "          5.8181e-01, -8.0496e-03,  6.8162e-01, -2.5719e-01,  8.4089e-01,\n",
      "          6.9056e-01, -8.9271e-01,  7.9174e-01, -7.2459e-01,  6.1102e-01],\n",
      "        [-4.4423e-01,  1.0000e+00, -4.7259e-02,  2.8673e-01, -1.0000e+00,\n",
      "         -1.0000e+00,  9.8531e-01,  1.0000e+00,  5.8550e-01, -5.8249e-01,\n",
      "          6.1849e-01,  4.4572e-02,  6.8070e-01, -2.3022e-01,  8.4594e-01,\n",
      "          6.9139e-01, -8.9603e-01,  7.9950e-01, -6.9791e-01,  6.0351e-01],\n",
      "        [-7.5322e-01,  1.0000e+00, -7.3768e-02,  2.1194e-01, -1.0000e+00,\n",
      "         -1.0000e+00,  9.8778e-01,  1.0000e+00,  6.3902e-01, -5.5622e-01,\n",
      "          6.3523e-01,  8.4427e-03,  6.8582e-01, -2.1038e-01,  8.3593e-01,\n",
      "          6.9710e-01, -8.9619e-01,  7.9240e-01, -6.8889e-01,  6.0078e-01],\n",
      "        [-4.9345e-01,  1.0000e+00,  6.5766e-03,  5.1921e-01, -1.0000e+00,\n",
      "         -1.0000e+00,  9.8768e-01,  1.0000e+00,  5.8698e-01, -5.2698e-01,\n",
      "          6.0503e-01,  2.9506e-03,  6.8361e-01, -2.1062e-01,  8.4060e-01,\n",
      "          6.8893e-01, -8.9588e-01,  7.8154e-01, -6.7393e-01,  5.9950e-01],\n",
      "        [-6.6881e-01,  1.0000e+00, -3.1412e-02,  4.4024e-01, -1.0000e+00,\n",
      "         -1.0000e+00,  9.9966e-01,  1.0000e+00,  6.1310e-01, -5.4831e-01,\n",
      "          6.2645e-01, -4.7085e-02,  6.8572e-01, -2.1408e-01,  8.4949e-01,\n",
      "          6.9958e-01, -8.9808e-01,  7.8380e-01, -7.1372e-01,  6.0635e-01],\n",
      "        [-4.1909e-01,  1.0000e+00, -5.1049e-02,  2.0179e-01, -1.0000e+00,\n",
      "         -1.0000e+00,  9.9965e-01,  1.0000e+00,  5.8603e-01, -6.2594e-01,\n",
      "          6.0999e-01,  1.7881e-02,  6.8255e-01, -2.6375e-01,  8.5572e-01,\n",
      "          7.0567e-01, -8.9712e-01,  8.1128e-01, -7.2798e-01,  6.2107e-01],\n",
      "        [-5.9088e-01,  1.0000e+00, -5.5411e-02,  4.0000e-02, -1.0000e+00,\n",
      "         -1.0000e+00,  9.9903e-01,  1.0000e+00,  6.5655e-01, -4.9866e-01,\n",
      "          6.3798e-01, -5.6846e-02,  6.8560e-01, -1.9666e-01,  8.4262e-01,\n",
      "          7.1566e-01, -8.9689e-01,  7.6546e-01, -6.8881e-01,  6.1189e-01],\n",
      "        [-5.4756e-01,  1.0000e+00, -6.1967e-02,  2.1020e-01, -1.0000e+00,\n",
      "         -1.0000e+00,  9.9875e-01,  1.0000e+00,  6.2215e-01, -5.8058e-01,\n",
      "          6.2502e-01,  1.6103e-02,  6.8376e-01, -2.2923e-01,  8.5519e-01,\n",
      "          7.0094e-01, -9.0102e-01,  8.0436e-01, -7.0306e-01,  6.1677e-01],\n",
      "        [-5.6992e-01,  1.0000e+00, -2.3950e-02,  1.4307e-01, -1.0000e+00,\n",
      "         -1.0000e+00,  9.6405e-01,  1.0000e+00,  5.6233e-01, -5.8091e-01,\n",
      "          6.3099e-01,  2.6197e-03,  6.8453e-01, -2.2936e-01,  8.5836e-01,\n",
      "          7.0488e-01, -9.0117e-01,  7.9474e-01, -6.9062e-01,  6.1103e-01],\n",
      "        [-5.3009e-01,  1.0000e+00, -3.0723e-02,  2.7771e-01, -1.0000e+00,\n",
      "         -1.0000e+00,  9.5569e-01,  1.0000e+00,  5.7490e-01, -5.5057e-01,\n",
      "          6.0230e-01,  4.9114e-02,  6.7852e-01, -2.1473e-01,  8.5003e-01,\n",
      "          6.9782e-01, -8.9235e-01,  8.0483e-01, -6.7455e-01,  6.0922e-01],\n",
      "        [-6.1798e-01,  1.0000e+00, -5.1503e-02,  3.5995e-01, -1.0000e+00,\n",
      "         -1.0000e+00,  9.9021e-01,  1.0000e+00,  6.6402e-01, -5.6831e-01,\n",
      "          6.1688e-01, -4.9039e-02,  6.7791e-01, -2.4120e-01,  8.3587e-01,\n",
      "          7.0723e-01, -8.9237e-01,  7.8437e-01, -7.1962e-01,  6.0575e-01],\n",
      "        [-3.7844e-01,  1.0000e+00, -2.9048e-02,  2.9428e-01, -1.0000e+00,\n",
      "         -1.0000e+00,  9.9372e-01,  1.0000e+00,  6.1822e-01, -5.0393e-01,\n",
      "          6.4621e-01, -4.1931e-03,  6.7718e-01, -2.0337e-01,  8.5033e-01,\n",
      "          7.0935e-01, -8.9779e-01,  7.8667e-01, -6.9938e-01,  6.1248e-01],\n",
      "        [-4.1601e-01,  1.0000e+00,  4.9452e-02,  1.2000e-01, -1.0000e+00,\n",
      "         -1.0000e+00,  9.9891e-01,  1.0000e+00,  5.9406e-01, -5.7535e-01,\n",
      "          5.8266e-01,  4.3930e-03,  6.7587e-01, -2.5639e-01,  8.5471e-01,\n",
      "          6.9942e-01, -8.9558e-01,  8.0197e-01, -7.1057e-01,  6.1511e-01],\n",
      "        [-6.7404e-01,  1.0000e+00,  4.7630e-02,  3.9309e-02, -1.0000e+00,\n",
      "         -1.0000e+00,  9.8595e-01,  1.0000e+00,  5.8779e-01, -6.3760e-01,\n",
      "          5.9105e-01,  3.1082e-02,  6.8193e-01, -2.6287e-01,  8.5132e-01,\n",
      "          7.0196e-01, -8.9649e-01,  8.0761e-01, -7.0490e-01,  6.0651e-01],\n",
      "        [-4.4523e-01,  1.0000e+00, -1.1603e-02,  2.8132e-01, -1.0000e+00,\n",
      "         -1.0000e+00,  9.9993e-01,  1.0000e+00,  6.2176e-01, -5.7226e-01,\n",
      "          6.1429e-01,  2.8279e-02,  6.8193e-01, -2.1611e-01,  8.4482e-01,\n",
      "          6.9787e-01, -8.9749e-01,  8.0521e-01, -6.9726e-01,  6.1018e-01],\n",
      "        [-6.5224e-01,  1.0000e+00,  4.0281e-02,  4.3995e-01, -1.0000e+00,\n",
      "         -1.0000e+00,  9.9975e-01,  1.0000e+00,  6.6267e-01, -5.1847e-01,\n",
      "          6.0498e-01, -3.2852e-02,  6.7972e-01, -2.0797e-01,  8.3537e-01,\n",
      "          6.9748e-01, -8.9450e-01,  7.9390e-01, -7.0511e-01,  6.0661e-01],\n",
      "        [-5.1558e-01,  1.0000e+00, -5.0434e-02,  2.1661e-01, -1.0000e+00,\n",
      "         -1.0000e+00,  9.8835e-01,  1.0000e+00,  5.7560e-01, -5.6287e-01,\n",
      "          6.7171e-01,  2.1279e-02,  6.8625e-01, -2.0386e-01,  8.5600e-01,\n",
      "          7.0326e-01, -9.0228e-01,  7.8934e-01, -6.8833e-01,  6.1152e-01],\n",
      "        [-6.4174e-01,  1.0000e+00, -7.9828e-02,  2.8032e-01, -1.0000e+00,\n",
      "         -1.0000e+00,  9.9974e-01,  1.0000e+00,  6.6740e-01, -6.2698e-01,\n",
      "          5.9974e-01,  2.2514e-02,  6.8551e-01, -2.3667e-01,  8.3939e-01,\n",
      "          6.9754e-01, -8.9532e-01,  8.1423e-01, -7.1877e-01,  6.1125e-01],\n",
      "        [-5.6957e-01,  1.0000e+00, -6.9399e-02,  2.8000e-01, -1.0000e+00,\n",
      "         -1.0000e+00,  9.9969e-01,  1.0000e+00,  6.4837e-01, -5.3300e-01,\n",
      "          6.1199e-01,  8.0501e-03,  6.8219e-01, -2.2560e-01,  8.4175e-01,\n",
      "          6.9821e-01, -9.0058e-01,  7.8713e-01, -6.8998e-01,  6.0448e-01],\n",
      "        [-5.1998e-01,  1.0000e+00, -6.5907e-02,  3.9899e-01, -1.0000e+00,\n",
      "         -1.0000e+00,  1.0000e+00,  1.0000e+00,  6.4840e-01, -4.8928e-01,\n",
      "          6.2930e-01, -1.5987e-02,  6.7582e-01, -2.0421e-01,  8.3871e-01,\n",
      "          7.0149e-01, -8.9789e-01,  7.7371e-01, -6.8137e-01,  6.0885e-01]])\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "2\n",
      "tensor([[ 0.9998, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  0.8165,  1.0000,\n",
      "         -0.5224, -0.6635,  0.5631,  0.0698, -0.3002, -0.2192, -0.3005, -0.0874,\n",
      "          0.0350, -0.1713,  0.1992,  0.2042],\n",
      "        [ 0.9823, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  0.8013,  1.0000,\n",
      "         -0.5255, -0.6612,  0.5666,  0.0706, -0.2999, -0.2192, -0.3017, -0.0870,\n",
      "          0.0349, -0.1721,  0.1986,  0.2045],\n",
      "        [ 0.9947, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  0.8228,  1.0000,\n",
      "         -0.5247, -0.6648,  0.5629,  0.0686, -0.3008, -0.2205, -0.3013, -0.0870,\n",
      "          0.0359, -0.1716,  0.1993,  0.2034],\n",
      "        [ 0.9999, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  0.8413,  1.0000,\n",
      "         -0.5223, -0.6638,  0.5615,  0.0685, -0.3002, -0.2196, -0.2999, -0.0876,\n",
      "          0.0354, -0.1711,  0.1991,  0.2038],\n",
      "        [ 1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  0.8388,  1.0000,\n",
      "         -0.5222, -0.6638,  0.5615,  0.0682, -0.3004, -0.2199, -0.3001, -0.0879,\n",
      "          0.0358, -0.1711,  0.1994,  0.2036],\n",
      "        [ 0.9986, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  0.8826,  1.0000,\n",
      "         -0.5202, -0.6621,  0.5586,  0.0667, -0.3004, -0.2186, -0.2992, -0.0877,\n",
      "          0.0353, -0.1698,  0.1985,  0.2038],\n",
      "        [ 1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  0.8442,  1.0000,\n",
      "         -0.5227, -0.6636,  0.5620,  0.0696, -0.2999, -0.2195, -0.2999, -0.0870,\n",
      "          0.0349, -0.1710,  0.1985,  0.2041],\n",
      "        [ 0.9982, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  0.8279,  1.0000,\n",
      "         -0.5229, -0.6624,  0.5636,  0.0701, -0.2995, -0.2190, -0.3005, -0.0869,\n",
      "          0.0345, -0.1711,  0.1983,  0.2046],\n",
      "        [ 0.9954, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  0.7897,  1.0000,\n",
      "         -0.5254, -0.6652,  0.5643,  0.0697, -0.3011, -0.2207, -0.3017, -0.0871,\n",
      "          0.0361, -0.1722,  0.2001,  0.2034],\n",
      "        [ 0.9592, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  0.8335,  1.0000,\n",
      "         -0.5249, -0.6588,  0.5669,  0.0686, -0.3015, -0.2188, -0.3028, -0.0872,\n",
      "          0.0358, -0.1717,  0.1984,  0.2034],\n",
      "        [ 0.9994, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  0.7848,  1.0000,\n",
      "         -0.5239, -0.6642,  0.5661,  0.0729, -0.2993, -0.2192, -0.3013, -0.0863,\n",
      "          0.0335, -0.1718,  0.1989,  0.2051],\n",
      "        [ 0.9992, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  0.7962,  1.0000,\n",
      "         -0.5241, -0.6645,  0.5654,  0.0708, -0.2999, -0.2199, -0.3011, -0.0871,\n",
      "          0.0351, -0.1720,  0.1994,  0.2043],\n",
      "        [ 0.9993, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  0.8254,  1.0000,\n",
      "         -0.5234, -0.6645,  0.5636,  0.0697, -0.3002, -0.2201, -0.3006, -0.0873,\n",
      "          0.0355, -0.1717,  0.1992,  0.2039],\n",
      "        [ 0.9998, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  0.8273,  1.0000,\n",
      "         -0.5230, -0.6643,  0.5633,  0.0696, -0.3001, -0.2198, -0.3006, -0.0871,\n",
      "          0.0354, -0.1713,  0.1991,  0.2039],\n",
      "        [ 0.9998, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  0.7936,  1.0000,\n",
      "         -0.5238, -0.6648,  0.5657,  0.0707, -0.3004, -0.2201, -0.3014, -0.0872,\n",
      "          0.0352, -0.1721,  0.1997,  0.2041],\n",
      "        [ 0.9741, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  0.8422,  1.0000,\n",
      "         -0.5240, -0.6589,  0.5653,  0.0707, -0.2998, -0.2179, -0.3010, -0.0864,\n",
      "          0.0340, -0.1712,  0.1971,  0.2046],\n",
      "        [ 0.9622, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  0.8890,  1.0000,\n",
      "         -0.5237, -0.6551,  0.5635,  0.0674, -0.3000, -0.2176, -0.3005, -0.0874,\n",
      "          0.0361, -0.1709,  0.1961,  0.2035],\n",
      "        [ 0.9991, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  0.8744,  1.0000,\n",
      "         -0.5209, -0.6635,  0.5605,  0.0662, -0.3007, -0.2197, -0.2996, -0.0884,\n",
      "          0.0367, -0.1705,  0.1996,  0.2031],\n",
      "        [ 0.9886, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  0.8473,  1.0000,\n",
      "         -0.5232, -0.6615,  0.5641,  0.0693, -0.2999, -0.2190, -0.3007, -0.0870,\n",
      "          0.0350, -0.1711,  0.1981,  0.2041],\n",
      "        [ 0.9999, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  0.8584,  1.0000,\n",
      "         -0.5218, -0.6637,  0.5614,  0.0675, -0.3005, -0.2198, -0.3000, -0.0878,\n",
      "          0.0361, -0.1708,  0.1991,  0.2034],\n",
      "        [ 0.9982, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  0.8004,  1.0000,\n",
      "         -0.5235, -0.6647,  0.5679,  0.0693, -0.3007, -0.2206, -0.3029, -0.0875,\n",
      "          0.0348, -0.1718,  0.1999,  0.2042],\n",
      "        [ 0.9998, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  0.7989,  1.0000,\n",
      "         -0.5240, -0.6634,  0.5664,  0.0714, -0.2995, -0.2197, -0.3014, -0.0867,\n",
      "          0.0349, -0.1718,  0.1991,  0.2045],\n",
      "        [ 1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  0.7632,  1.0000,\n",
      "         -0.5254, -0.6653,  0.5675,  0.0716, -0.3001, -0.2206, -0.3019, -0.0872,\n",
      "          0.0353, -0.1728,  0.2001,  0.2042],\n",
      "        [ 1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  0.8166,  1.0000,\n",
      "         -0.5223, -0.6652,  0.5639,  0.0675, -0.3012, -0.2205, -0.3012, -0.0884,\n",
      "          0.0365, -0.1717,  0.2005,  0.2032],\n",
      "        [ 0.9957, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  0.7950,  1.0000,\n",
      "         -0.5245, -0.6647,  0.5660,  0.0701, -0.3008, -0.2204, -0.3018, -0.0875,\n",
      "          0.0355, -0.1723,  0.2000,  0.2038],\n",
      "        [ 1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  0.7598,  1.0000,\n",
      "         -0.5252, -0.6649,  0.5674,  0.0721, -0.2999, -0.2204, -0.3021, -0.0869,\n",
      "          0.0348, -0.1727,  0.1998,  0.2046],\n",
      "        [ 0.9602, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  0.8474,  1.0000,\n",
      "         -0.5245, -0.6544,  0.5667,  0.0696, -0.2996, -0.2172, -0.3017, -0.0869,\n",
      "          0.0351, -0.1715,  0.1961,  0.2043],\n",
      "        [ 1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  0.8243,  1.0000,\n",
      "         -0.5233, -0.6632,  0.5636,  0.0694, -0.2998, -0.2198, -0.3004, -0.0876,\n",
      "          0.0359, -0.1716,  0.1994,  0.2040],\n",
      "        [ 0.9998, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  0.8223,  1.0000,\n",
      "         -0.5237, -0.6648,  0.5636,  0.0690, -0.3005, -0.2206, -0.3007, -0.0876,\n",
      "          0.0360, -0.1719,  0.1996,  0.2036],\n",
      "        [ 0.9990, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  0.8292,  1.0000,\n",
      "         -0.5235, -0.6634,  0.5638,  0.0702, -0.2998, -0.2197, -0.3005, -0.0870,\n",
      "          0.0350, -0.1714,  0.1986,  0.2042],\n",
      "        [ 0.9891, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  0.8094,  1.0000,\n",
      "         -0.5255, -0.6629,  0.5663,  0.0715, -0.2997, -0.2197, -0.3015, -0.0863,\n",
      "          0.0344, -0.1719,  0.1983,  0.2044],\n",
      "        [ 0.9996, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  0.9016,  1.0000,\n",
      "         -0.5196, -0.6629,  0.5582,  0.0644, -0.3012, -0.2196, -0.2993, -0.0887,\n",
      "          0.0373, -0.1700,  0.1993,  0.2026]])\n",
      "tensor([[ 0.9998, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  0.8165,  1.0000,\n",
      "         -0.5224, -0.6635,  0.5631,  0.0698, -0.3002, -0.2192, -0.3005, -0.0874,\n",
      "          0.0350, -0.1713,  0.1992,  0.2042],\n",
      "        [ 0.9823, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  0.8013,  1.0000,\n",
      "         -0.5255, -0.6612,  0.5666,  0.0706, -0.2999, -0.2192, -0.3017, -0.0870,\n",
      "          0.0349, -0.1721,  0.1986,  0.2045],\n",
      "        [ 0.9947, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  0.8228,  1.0000,\n",
      "         -0.5247, -0.6648,  0.5629,  0.0686, -0.3008, -0.2205, -0.3013, -0.0870,\n",
      "          0.0359, -0.1716,  0.1993,  0.2034],\n",
      "        [ 0.9999, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  0.8413,  1.0000,\n",
      "         -0.5223, -0.6638,  0.5615,  0.0685, -0.3002, -0.2196, -0.2999, -0.0876,\n",
      "          0.0354, -0.1711,  0.1991,  0.2038],\n",
      "        [ 1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  0.8388,  1.0000,\n",
      "         -0.5222, -0.6638,  0.5615,  0.0682, -0.3004, -0.2199, -0.3001, -0.0879,\n",
      "          0.0358, -0.1711,  0.1994,  0.2036],\n",
      "        [ 0.9986, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  0.8826,  1.0000,\n",
      "         -0.5202, -0.6621,  0.5586,  0.0667, -0.3004, -0.2186, -0.2992, -0.0877,\n",
      "          0.0353, -0.1698,  0.1985,  0.2038],\n",
      "        [ 1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  0.8442,  1.0000,\n",
      "         -0.5227, -0.6636,  0.5620,  0.0696, -0.2999, -0.2195, -0.2999, -0.0870,\n",
      "          0.0349, -0.1710,  0.1985,  0.2041],\n",
      "        [ 0.9982, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  0.8279,  1.0000,\n",
      "         -0.5229, -0.6624,  0.5636,  0.0701, -0.2995, -0.2190, -0.3005, -0.0869,\n",
      "          0.0345, -0.1711,  0.1983,  0.2046],\n",
      "        [ 0.9954, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  0.7897,  1.0000,\n",
      "         -0.5254, -0.6652,  0.5643,  0.0697, -0.3011, -0.2207, -0.3017, -0.0871,\n",
      "          0.0361, -0.1722,  0.2001,  0.2034],\n",
      "        [ 0.9592, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  0.8335,  1.0000,\n",
      "         -0.5249, -0.6588,  0.5669,  0.0686, -0.3015, -0.2188, -0.3028, -0.0872,\n",
      "          0.0358, -0.1717,  0.1984,  0.2034],\n",
      "        [ 0.9994, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  0.7848,  1.0000,\n",
      "         -0.5239, -0.6642,  0.5661,  0.0729, -0.2993, -0.2192, -0.3013, -0.0863,\n",
      "          0.0335, -0.1718,  0.1989,  0.2051],\n",
      "        [ 0.9992, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  0.7962,  1.0000,\n",
      "         -0.5241, -0.6645,  0.5654,  0.0708, -0.2999, -0.2199, -0.3011, -0.0871,\n",
      "          0.0351, -0.1720,  0.1994,  0.2043],\n",
      "        [ 0.9993, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  0.8254,  1.0000,\n",
      "         -0.5234, -0.6645,  0.5636,  0.0697, -0.3002, -0.2201, -0.3006, -0.0873,\n",
      "          0.0355, -0.1717,  0.1992,  0.2039],\n",
      "        [ 0.9998, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  0.8273,  1.0000,\n",
      "         -0.5230, -0.6643,  0.5633,  0.0696, -0.3001, -0.2198, -0.3006, -0.0871,\n",
      "          0.0354, -0.1713,  0.1991,  0.2039],\n",
      "        [ 0.9998, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  0.7936,  1.0000,\n",
      "         -0.5238, -0.6648,  0.5657,  0.0707, -0.3004, -0.2201, -0.3014, -0.0872,\n",
      "          0.0352, -0.1721,  0.1997,  0.2041],\n",
      "        [ 0.9741, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  0.8422,  1.0000,\n",
      "         -0.5240, -0.6589,  0.5653,  0.0707, -0.2998, -0.2179, -0.3010, -0.0864,\n",
      "          0.0340, -0.1712,  0.1971,  0.2046],\n",
      "        [ 0.9622, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  0.8890,  1.0000,\n",
      "         -0.5237, -0.6551,  0.5635,  0.0674, -0.3000, -0.2176, -0.3005, -0.0874,\n",
      "          0.0361, -0.1709,  0.1961,  0.2035],\n",
      "        [ 0.9991, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  0.8744,  1.0000,\n",
      "         -0.5209, -0.6635,  0.5605,  0.0662, -0.3007, -0.2197, -0.2996, -0.0884,\n",
      "          0.0367, -0.1705,  0.1996,  0.2031],\n",
      "        [ 0.9886, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  0.8473,  1.0000,\n",
      "         -0.5232, -0.6615,  0.5641,  0.0693, -0.2999, -0.2190, -0.3007, -0.0870,\n",
      "          0.0350, -0.1711,  0.1981,  0.2041],\n",
      "        [ 0.9999, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  0.8584,  1.0000,\n",
      "         -0.5218, -0.6637,  0.5614,  0.0675, -0.3005, -0.2198, -0.3000, -0.0878,\n",
      "          0.0361, -0.1708,  0.1991,  0.2034],\n",
      "        [ 0.9982, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  0.8004,  1.0000,\n",
      "         -0.5235, -0.6647,  0.5679,  0.0693, -0.3007, -0.2206, -0.3029, -0.0875,\n",
      "          0.0348, -0.1718,  0.1999,  0.2042],\n",
      "        [ 0.9998, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  0.7989,  1.0000,\n",
      "         -0.5240, -0.6634,  0.5664,  0.0714, -0.2995, -0.2197, -0.3014, -0.0867,\n",
      "          0.0349, -0.1718,  0.1991,  0.2045],\n",
      "        [ 1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  0.7632,  1.0000,\n",
      "         -0.5254, -0.6653,  0.5675,  0.0716, -0.3001, -0.2206, -0.3019, -0.0872,\n",
      "          0.0353, -0.1728,  0.2001,  0.2042],\n",
      "        [ 1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  0.8166,  1.0000,\n",
      "         -0.5223, -0.6652,  0.5639,  0.0675, -0.3012, -0.2205, -0.3012, -0.0884,\n",
      "          0.0365, -0.1717,  0.2005,  0.2032],\n",
      "        [ 0.9957, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  0.7950,  1.0000,\n",
      "         -0.5245, -0.6647,  0.5660,  0.0701, -0.3008, -0.2204, -0.3018, -0.0875,\n",
      "          0.0355, -0.1723,  0.2000,  0.2038],\n",
      "        [ 1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  0.7598,  1.0000,\n",
      "         -0.5252, -0.6649,  0.5674,  0.0721, -0.2999, -0.2204, -0.3021, -0.0869,\n",
      "          0.0348, -0.1727,  0.1998,  0.2046],\n",
      "        [ 0.9602, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  0.8474,  1.0000,\n",
      "         -0.5245, -0.6544,  0.5667,  0.0696, -0.2996, -0.2172, -0.3017, -0.0869,\n",
      "          0.0351, -0.1715,  0.1961,  0.2043],\n",
      "        [ 1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  0.8243,  1.0000,\n",
      "         -0.5233, -0.6632,  0.5636,  0.0694, -0.2998, -0.2198, -0.3004, -0.0876,\n",
      "          0.0359, -0.1716,  0.1994,  0.2040],\n",
      "        [ 0.9998, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  0.8223,  1.0000,\n",
      "         -0.5237, -0.6648,  0.5636,  0.0690, -0.3005, -0.2206, -0.3007, -0.0876,\n",
      "          0.0360, -0.1719,  0.1996,  0.2036],\n",
      "        [ 0.9990, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  0.8292,  1.0000,\n",
      "         -0.5235, -0.6634,  0.5638,  0.0702, -0.2998, -0.2197, -0.3005, -0.0870,\n",
      "          0.0350, -0.1714,  0.1986,  0.2042],\n",
      "        [ 0.9891, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  0.8094,  1.0000,\n",
      "         -0.5255, -0.6629,  0.5663,  0.0715, -0.2997, -0.2197, -0.3015, -0.0863,\n",
      "          0.0344, -0.1719,  0.1983,  0.2044],\n",
      "        [ 0.9996, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  0.9016,  1.0000,\n",
      "         -0.5196, -0.6629,  0.5582,  0.0644, -0.3012, -0.2196, -0.2993, -0.0887,\n",
      "          0.0373, -0.1700,  0.1993,  0.2026]])\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "cumModel = hiddens[0][2]\n",
    "iter = 1\n",
    "for model in hiddens[1:]:\n",
    "    if model[1] != -1:\n",
    "        if iter == 0:\n",
    "            cumModel = model[2]\n",
    "        else:\n",
    "            cumModel = cumModel + model[2]\n",
    "        iter += 1\n",
    "    else:\n",
    "        print(model[0])\n",
    "        print(cumModel/iter)\n",
    "        print(model[2])\n",
    "        print(cumModel/iter - model[2])\n",
    "        iter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "25ec7392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "list = [0,1,2]\n",
    "print(list[0:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1d50dd45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "testOBU = OBU(\n",
    "    inputSize = 9,  # Number of features per BSM\n",
    "    units = 20, # Number of hidden cells\n",
    "    motors = 8, # Number of motor neurons\n",
    "    outputs = 20, # Number of possible labels\n",
    "    epochs = 100,\n",
    "    lr = 0.001,\n",
    "    gpu = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a27e9858",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m testOBU\u001b[38;5;241m.\u001b[39mfit(\u001b[43mdataSets\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m54\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "testOBU.fit(dataSets[54])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "382a51c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "save = OBU(testOBU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b2a6e7e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([19700, 10, 20])\n",
      "Model got 114509/197000 right. Accuracy of 58.126395939086294%\n",
      "41.08375634517766% Zeroes.\n"
     ]
    }
   ],
   "source": [
    "_, _, perc, _ = testOBU.test(inTest, outTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7c51f439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([196990, 10, 20])\n",
      "Model got 1159988/1969900 right. Accuracy of 58.88562871211737%\n",
      "40.353165135286055% Zeroes.\n"
     ]
    }
   ],
   "source": [
    "_, _, perc, _ = testOBU.test(testDataIn, testDataOut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0e5f9608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([19700, 10, 20])\n",
      "Model got 116065/197000 right. Accuracy of 58.91624365482233%\n",
      "41.08375634517766% Zeroes.\n"
     ]
    }
   ],
   "source": [
    "_, _, perc, _ = save.test(inTest, outTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1a959600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(testOBU.getHidden() - save.getHidden())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9f79ef7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(testOBU.model.fF.weight - save.model.fF.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0097f187",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kettering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
