{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a997a78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports:\n",
    "from ncps.wirings import AutoNCP\n",
    "from ncps.torch import CfC\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from numpy import genfromtxt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import matplotlib as plt\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.set_float32_matmul_precision(\"high\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2491f35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Base File\n",
    "#Current Simulation File\n",
    "dataFile = 'Data/CfCMultiExtension/DoS_0709.csv'\n",
    "\n",
    "dataSet = genfromtxt(dataFile, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "65c814c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[81], line 8\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#PROPER FORMATTING\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#Time sequences are 10 timepoints (Messages) with 7 features per message.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#Organized by car.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#Current Simulation File\u001b[39;00m\n\u001b[0;32m      6\u001b[0m dataFile \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mData/CfCMultiExtension/DoS_0709.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 8\u001b[0m dataSet \u001b[38;5;241m=\u001b[39m \u001b[43mgenfromtxt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataFile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m batchSize \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Ceate dataloader and fill with (BSM, attk#). Expanding to add 0th dimension for batches.\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Batch size should be 64 for the low density simulations and 128 for high density simulations.\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# No shuffle to keep batches on same vehicle.\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Num_workers is set to = num CPU cores\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\will\\miniconda3\\envs\\Kettering\\Lib\\site-packages\\numpy\\lib\\_npyio_impl.py:2349\u001b[0m, in \u001b[0;36mgenfromtxt\u001b[1;34m(fname, dtype, comments, delimiter, skip_header, skip_footer, converters, missing_values, filling_values, usecols, names, excludelist, deletechars, replace_space, autostrip, case_sensitive, defaultfmt, unpack, usemask, loose, invalid_raise, max_rows, encoding, ndmin, like)\u001b[0m\n\u001b[0;32m   2345\u001b[0m \u001b[38;5;66;03m# Convert each value according to the converter:\u001b[39;00m\n\u001b[0;32m   2346\u001b[0m \u001b[38;5;66;03m# We want to modify the list in place to avoid creating a new one...\u001b[39;00m\n\u001b[0;32m   2347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loose:\n\u001b[0;32m   2348\u001b[0m     rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m-> 2349\u001b[0m         \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m[[\u001b[43mconv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_loose_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_r\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m _r \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mmap\u001b[39m(itemgetter(i), rows)]\n\u001b[0;32m   2350\u001b[0m               \u001b[38;5;28;01mfor\u001b[39;00m (i, conv) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(converters)]))\n\u001b[0;32m   2351\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2352\u001b[0m     rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[0;32m   2353\u001b[0m         \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m[[conv\u001b[38;5;241m.\u001b[39m_strict_call(_r) \u001b[38;5;28;01mfor\u001b[39;00m _r \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mmap\u001b[39m(itemgetter(i), rows)]\n\u001b[0;32m   2354\u001b[0m               \u001b[38;5;28;01mfor\u001b[39;00m (i, conv) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(converters)]))\n",
      "File \u001b[1;32mc:\\Users\\will\\miniconda3\\envs\\Kettering\\Lib\\site-packages\\numpy\\lib\\_iotools.py:674\u001b[0m, in \u001b[0;36mStringConverter._loose_call\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_loose_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, value):\n\u001b[0;32m    673\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 674\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[0;32m    676\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#PROPER FORMATTING\n",
    "#Time sequences are 10 timepoints (Messages) with 7 features per message.\n",
    "#Organized by car.\n",
    "\n",
    "batchSize = 64\n",
    "# Ceate dataloader and fill with (BSM, attk#). Expanding to add 0th dimension for batches.\n",
    "# Batch size should be 64 for the low density simulations and 128 for high density simulations.\n",
    "# No shuffle to keep batches on same vehicle.\n",
    "# Num_workers is set to = num CPU cores\n",
    "dataSet[0:-1,:] = dataSet[1:,:] # Get rid of the first null value of the dataset\n",
    "# Sort Dataset by reciever id to pass on to every car.\n",
    "dataSet = dataSet[np.argsort(dataSet[:, 1])]\n",
    "print(dataSet.shape)\n",
    "# count subsets per vehicle\n",
    "unq, counts = np.unique(dataSet[:, 1], return_counts = True)\n",
    "recvr = 0\n",
    "lastRecieverCount = 0\n",
    "newData = []\n",
    "# Organize dataset into sets of 10 messages by reciever\n",
    "while recvr < counts.shape[0]:\n",
    "    # Loop through reciever\n",
    "    index = 0\n",
    "    while index < counts[recvr] - 10:\n",
    "        # Loop through messages from reciever\n",
    "        newData.append(dataSet[lastRecieverCount+index:lastRecieverCount +index+10])\n",
    "        index += 5\n",
    "    recvr += 1\n",
    "    lastRecieverCount = counts[recvr-1]\n",
    "dataSet = torch.tensor(newData)\n",
    "len = dataSet.shape[0]\n",
    "trainPerc = 80\n",
    "# Create new arrays per vehicle for federated learning\n",
    "splits = np.split(dataSet, np.cumsum(counts)[:-1])\n",
    "# Create seperate datasets for testing and training, using Train Percentage as metric for split\n",
    "trainDataIn = torch.Tensor(dataSet[:int(len*(trainPerc/100)),:,1:10]).float()\n",
    "trainDataOut = torch.Tensor(np.int_(dataSet[:int(len*(trainPerc/100)),:,11])).long()\n",
    "testDataIn = torch.Tensor(dataSet[int(len*(trainPerc/100)):,:,1:10]).float()\n",
    "testDataOut = torch.Tensor(np.int_(dataSet[int(len*(trainPerc/100)):,:,11])).long()\n",
    "newsetIn = []\n",
    "newsetOut = []\n",
    "testsetIn = []\n",
    "testsetOut = []\n",
    "# Create dataset of 1/100th of the entries for quicker testing during development\n",
    "for index in range(0,int((len) * (trainPerc/100))):\n",
    "    if not (int(index/10) % 100):\n",
    "        newsetIn.append(dataSet[index,:,1:10])\n",
    "        newsetOut.append((dataSet[index,:,11]))\n",
    "for idx in range(int((len) * (trainPerc/100)), len):\n",
    "    if not (int(index/10) % 10):   \n",
    "        print(\"TOM\")\n",
    "        testsetIn.append(dataSet[index,:,1:10])\n",
    "        testsetOut.append((dataSet[index,:,11]))\n",
    "testingIn = torch.Tensor(np.array(newsetIn)).float()\n",
    "testingOut = torch.Tensor(np.array(newsetOut)).long()\n",
    "inTest = torch.Tensor(np.array(testsetIn)).float()\n",
    "outTest = torch.Tensor(np.array(testsetOut)).long()\n",
    "# Create Dataloaders for all the datasets\n",
    "dataLoaderTrain = data.DataLoader(data.TensorDataset(trainDataIn, trainDataOut), batch_size=batchSize, shuffle=False, num_workers=16, persistent_workers = True, drop_last= True)\n",
    "dataLoaderTest = data.DataLoader(data.TensorDataset(testDataIn, testDataOut), batch_size=batchSize, shuffle=False, num_workers=16, persistent_workers = True, drop_last= True)\n",
    "testingDataLoader = data.DataLoader(data.TensorDataset(testingIn, testingOut), batch_size=batchSize, shuffle = False, num_workers=16, persistent_workers = True, drop_last= True)\n",
    "testingTestData = data.DataLoader(data.TensorDataset(testingIn, testingOut), batch_size=batchSize, shuffle = False, num_workers=16, persistent_workers = True, drop_last= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3b158270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([0])\n"
     ]
    }
   ],
   "source": [
    "print(inTest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1da521c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3 4]\n",
      " [9 8 7 6]\n",
      " [5 7 8 3]\n",
      " [9 8 7 3]]\n",
      "[[1 2 3 4]\n",
      " [9 8 7 6]\n",
      " [9 8 7 3]\n",
      " [5 7 8 3]]\n"
     ]
    }
   ],
   "source": [
    "#TESTING\n",
    "tom = np.array([[1,2,3,4],[9,8,7,6],[5,7,8,3],[9,8,7,3]])\n",
    "print(tom)\n",
    "tom = tom[np.argsort(tom[:, 2])]\n",
    "print(tom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "01abeb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Learner\n",
    "class CfCLearner(pl.LightningModule):\n",
    "    def __init__(self, model, lr):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.lossFunc = nn.CrossEntropyLoss()\n",
    "        self.hidden = None\n",
    "\n",
    "    def setHidden(self, hid):\n",
    "        self.hidden = hid\n",
    "\n",
    "    def getHidden(self):\n",
    "        return self.hidden\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Get in and out from batch\n",
    "        inputs, target = batch\n",
    "        # Put input through model\n",
    "        output, hx = self.model.forward(inputs, self.hidden)\n",
    "        self.setHidden(hx.detach())\n",
    "        # Reorganize inputs for use with loss function\n",
    "        output = output.permute(0, 2, 1)\n",
    "        # Calculate Loss using Cross Entropy Loss \n",
    "        loss = self.lossFunc(output, target)\n",
    "        self.log(\"trainLoss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # Get in and out from batch\n",
    "        inputs, target = batch\n",
    "        # Put input through model\n",
    "        output, hx = self.model.forward(inputs, self.hidden)\n",
    "        self.setHidden(hx.detach())\n",
    "        # Reorganize inputs for use with loss function\n",
    "        output = output.permute(0, 2, 1)\n",
    "        print(f\"output: {output.shape}\")\n",
    "        print(f\"target: {target.shape}\")\n",
    "        # Calculate Loss using Cross Entropy Loss \n",
    "        loss = self.lossFunc(output, target)\n",
    "        self.log(\"valLoss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self.validation_step(batch, batch_idx)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Using AdamW optomizer based on info from paper\n",
    "        return torch.optim.AdamW(self.model.parameters(), lr = self.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bfb6cf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Modena(nn.Module): \n",
    "    # CfC with feed-forward layer to classify at end.\n",
    "    def __init__(self, inputSize, unitNum = None, motorNum = 2, outputDim = 2, batchFirst = True):\n",
    "        super().__init__()\n",
    "        if isinstance(inputSize, Modena):\n",
    "            self.inputSize = inputSize.inputSize\n",
    "            self.unitNum = inputSize.unitNum\n",
    "            self.motorNum = inputSize.motorNum\n",
    "            self.outputDim = inputSize.outputDim\n",
    "            self.batchFirst = inputSize.batchFirst\n",
    "            # Create NCP wiring for CfC\n",
    "            wiring = AutoNCP(self.unitNum, self.motorNum)\n",
    "            # Create CfC model with inputs and wiring\n",
    "            self.cfc = CfC(self.inputSize, wiring, batch_first=self.batchFirst)\n",
    "            # Create feed-forward layer\n",
    "            self.fF = nn.Linear(self.motorNum, self.outputDim)\n",
    "            self.fF.weight = nn.Parameter(inputSize.fF.weight)\n",
    "        else:\n",
    "            self.inputSize = inputSize\n",
    "            self.unitNum = unitNum\n",
    "            self.motorNum = motorNum\n",
    "            self.outputDim = outputDim\n",
    "            self.batchFirst = batchFirst\n",
    "            # Create NCP wiring for CfC\n",
    "            wiring = AutoNCP(unitNum, motorNum)\n",
    "            # Create CfC model with inputs and wiring\n",
    "            self.cfc = CfC(inputSize, wiring, batch_first=batchFirst)\n",
    "            # Create feed-forward layer\n",
    "            self.fF = nn.Linear(motorNum, outputDim)\n",
    "        \n",
    "\n",
    "    def forward(self, batch, hidden = None):\n",
    "        batch, hidden = self.cfc(batch, hidden) # Pass inputs through CfC\n",
    "        out = nn.functional.relu(self.fF(batch)) # pass through FeedForward Layer, then make 0 minimum\n",
    "        return out, hidden # Return the guess and the hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5406ad4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OBU module class to organize\n",
    "class OBU():\n",
    "    def __init__(self, inputSize, units = 20, motors = 8, outputs = 20, epochs = 10, lr = 0.001, gpu = False):\n",
    "        if isinstance(inputSize, OBU):\n",
    "            self.lr = inputSize.lr\n",
    "            self.epochs = inputSize.epochs\n",
    "            self.gpu = inputSize.gpu\n",
    "            self.model = Modena(inputSize.model)\n",
    "            self.learner = CfCLearner(self.model, self.lr)\n",
    "            self.trainer = pl.Trainer(\n",
    "                logger = CSVLogger('log'), # Set ouput destination of logs, logging accuracy every 50 steps\n",
    "                max_epochs = self.epochs, # Number of epochs to train for\n",
    "                gradient_clip_val = 1, # This is said to stabilize training, but we should test if that is true\n",
    "                accelerator = \"gpu\" if self.gpu else \"cpu\" # Using the GPU to run training or not\n",
    "                )\n",
    "            self.hidden = inputSize.hidden\n",
    "            self.learner.setHidden(inputSize.learner.getHidden())\n",
    "        else:\n",
    "            self.lr = lr\n",
    "            self.epochs = epochs\n",
    "            self.gpu = gpu\n",
    "            self.model = Modena(inputSize, units, motors, outputs)\n",
    "            self.learner = CfCLearner(self.model, lr) # tune units, lr\n",
    "            self.trainer = pl.Trainer(\n",
    "                logger = CSVLogger('log'), # Set ouput destination of logs, logging accuracy every 50 steps\n",
    "                max_epochs = epochs, # Number of epochs to train for\n",
    "                gradient_clip_val = 1, # This is said to stabilize training, but we should test if that is true\n",
    "                accelerator = \"gpu\" if gpu else \"cpu\" # Using the GPU to run training or not\n",
    "                )\n",
    "            self.hidden = None\n",
    "    \n",
    "        # Overloading add function to create fed.avg. model\n",
    "    def __add__(self, other):\n",
    "        if self.learner.getHidden() != None and other.learner.getHidden() != None:\n",
    "            self.learner.setHidden(self.learner.getHidden() +  other.learner.getHidden())\n",
    "            self.model.fF.weight = nn.Parameter(self.model.fF.weight + other.model.fF.weight)\n",
    "        elif other.learner.getHidden() != None:\n",
    "            self.learner.setHidden(other.learner.getHidden())\n",
    "            self.model.fF.weight = nn.Parameter(other.model.fF.weight)\n",
    "        elif self.learner.getHidden() != None:\n",
    "            self.learner.setHidden(self.learner.getHidden())\n",
    "            self.model.fF.weight = nn.Parameter(self.model.fF.weight)\n",
    "        return self\n",
    "\n",
    "    # Overloading div. function to average model\n",
    "    def __truediv__(self, i):\n",
    "        self.learner.setHidden(self.learner.getHidden() / i)\n",
    "        self.model.fF.weight = nn.Parameter(self.model.fF.weight/i)\n",
    "        return self\n",
    "    \n",
    "    def fit(self, dataLoader):\n",
    "        # calling built in fit function\n",
    "        return self.trainer.fit(self.learner, dataLoader)\n",
    "    \n",
    "    # Function to run model through a testing dataset and calculate accuracy. Can be expanded to give more metrics and more useful metrics.\n",
    "    def test(self, dataIn, dataOut, extraLayer = True):\n",
    "        # Put input data through model and determine classification\n",
    "        with torch.no_grad():\n",
    "            outs = self.model(dataIn)\n",
    "        if extraLayer:\n",
    "            outs = outs[0]\n",
    "        outs = np.asarray(outs)\n",
    "        outs = torch.from_numpy(outs)\n",
    "        # Get the label with the maximum confidence for determining classification\n",
    "        print(outs.shape)\n",
    "        _, res = torch.max(outs, 2)\n",
    "        countR = 0\n",
    "        numZero = 0\n",
    "        tot = outs.shape[0]\n",
    "        total = 0\n",
    "        for i in range(0, tot):\n",
    "            # Loop through sequences of 10 each\n",
    "            for t in range(0, res[i].shape[0]):\n",
    "                # Loop through the sub-sequences\n",
    "                if res[i,t] == dataOut[i,t]:\n",
    "                    # Check if label is correct, and add to count right accordingly\n",
    "                    countR += 1\n",
    "                if dataOut[i,t] == 0:\n",
    "                    # If the label is zero, increment the count of zeroes to determine if model is just outputting zeroes\n",
    "                    numZero += 1\n",
    "                total += 1\n",
    "        # Calculate percent correct and percent zero\n",
    "        perc = (countR/total) * 100\n",
    "        percZero = (numZero/total) * 100\n",
    "        print(\"Model got \" + str(countR) + \"/\" + str(total) + \" right. Accuracy of \" + str(perc) + \"%\")\n",
    "        print(str(percZero) + \"% Zeroes.\")\n",
    "        return countR, total, perc, percZero\n",
    "    \n",
    "    def testStep(self, dataLoader):\n",
    "        self.learner.validation_step(next(iter(dataLoader)), 0)\n",
    "\n",
    "    # Function to set hidden values of NCP wiring.\n",
    "    def setHidden(self, val):\n",
    "        self.learner.setHidden(val)\n",
    "\n",
    "    # Function to return hidden values of NCP wiring.\n",
    "    def getHidden(self):\n",
    "        return self.learner.getHidden()\n",
    "    \n",
    "    def setModel(self, model):\n",
    "        if not model == None:\n",
    "            self.model = model\n",
    "\n",
    "    def getModel(self):\n",
    "        return self.model\n",
    "    \n",
    "    def resetTrainer(self):\n",
    "        self.trainer = pl.Trainer(\n",
    "            logger = CSVLogger('log'), # Set ouput destination of logs, logging accuracy every 50 steps\n",
    "            max_epochs = self.epochs, # Number of epochs to train for\n",
    "            gradient_clip_val = 1, # This is said to stabilize training, but we should test if that is true\n",
    "            accelerator = \"gpu\" if self.gpu else \"cpu\" # Using the GPU to run training or not\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "cd58282c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\will\\miniconda3\\envs\\Kettering\\Lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "\n",
      "  | Name     | Type             | Params | Mode \n",
      "------------------------------------------------------\n",
      "0 | model    | Modena           | 1.7 K  | train\n",
      "1 | lossFunc | CrossEntropyLoss | 0      | train\n",
      "------------------------------------------------------\n",
      "1.4 K     Trainable params\n",
      "280       Non-trainable params\n",
      "1.7 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "27        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "c:\\Users\\will\\miniconda3\\envs\\Kettering\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:310: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 5/5 [00:00<00:00, 21.97it/s, v_num=306, trainLoss=2.620]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 5/5 [00:00<00:00, 20.17it/s, v_num=306, trainLoss=2.620]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([196990, 10, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name     | Type             | Params | Mode \n",
      "------------------------------------------------------\n",
      "0 | model    | Modena           | 1.7 K  | train\n",
      "1 | lossFunc | CrossEntropyLoss | 0      | train\n",
      "------------------------------------------------------\n",
      "1.4 K     Trainable params\n",
      "280       Non-trainable params\n",
      "1.7 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "27        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model got 794917/1969900 right. Accuracy of 40.353165135286055%\n",
      "40.353165135286055% Zeroes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\will\\miniconda3\\envs\\Kettering\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:106: Total length of `DataLoader` across ranks is zero. Please make sure this was your intention.\n",
      "c:\\Users\\will\\miniconda3\\envs\\Kettering\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:106: Total length of `CombinedLoader` across ranks is zero. Please make sure this was your intention.\n",
      "`Trainer.fit` stopped: No training batches.\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([196990, 10, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name     | Type             | Params | Mode \n",
      "------------------------------------------------------\n",
      "0 | model    | Modena           | 1.7 K  | train\n",
      "1 | lossFunc | CrossEntropyLoss | 0      | train\n",
      "------------------------------------------------------\n",
      "1.4 K     Trainable params\n",
      "280       Non-trainable params\n",
      "1.7 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "27        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model got 0/1969900 right. Accuracy of 0.0%\n",
      "40.353165135286055% Zeroes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\will\\miniconda3\\envs\\Kettering\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:310: The number of training batches (10) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 10/10 [00:00<00:00, 23.49it/s, v_num=308, trainLoss=2.000]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 10/10 [00:00<00:00, 22.59it/s, v_num=308, trainLoss=2.000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([196990, 10, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name     | Type             | Params | Mode \n",
      "------------------------------------------------------\n",
      "0 | model    | Modena           | 1.7 K  | train\n",
      "1 | lossFunc | CrossEntropyLoss | 0      | train\n",
      "------------------------------------------------------\n",
      "1.4 K     Trainable params\n",
      "280       Non-trainable params\n",
      "1.7 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "27        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model got 794917/1969900 right. Accuracy of 40.353165135286055%\n",
      "40.353165135286055% Zeroes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\will\\miniconda3\\envs\\Kettering\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:310: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 3/3 [00:00<00:00, 22.78it/s, v_num=309, trainLoss=2.880]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 3/3 [00:00<00:00, 20.08it/s, v_num=309, trainLoss=2.880]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([196990, 10, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name     | Type             | Params | Mode \n",
      "------------------------------------------------------\n",
      "0 | model    | Modena           | 1.7 K  | train\n",
      "1 | lossFunc | CrossEntropyLoss | 0      | train\n",
      "------------------------------------------------------\n",
      "1.4 K     Trainable params\n",
      "280       Non-trainable params\n",
      "1.7 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "27        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model got 98543/1969900 right. Accuracy of 5.00243667191228%\n",
      "40.353165135286055% Zeroes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\will\\miniconda3\\envs\\Kettering\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:310: The number of training batches (9) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 9/9 [00:00<00:00, 22.65it/s, v_num=310, trainLoss=1.950]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 9/9 [00:00<00:00, 21.69it/s, v_num=310, trainLoss=1.950]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([196990, 10, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name     | Type             | Params | Mode \n",
      "------------------------------------------------------\n",
      "0 | model    | Modena           | 1.7 K  | train\n",
      "1 | lossFunc | CrossEntropyLoss | 0      | train\n",
      "------------------------------------------------------\n",
      "1.4 K     Trainable params\n",
      "280       Non-trainable params\n",
      "1.7 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "27        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model got 797075/1969900 right. Accuracy of 40.4627138433423%\n",
      "40.353165135286055% Zeroes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\will\\miniconda3\\envs\\Kettering\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:310: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 1/1 [00:00<00:00, 16.55it/s, v_num=311, trainLoss=2.830]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 1/1 [00:00<00:00, 12.20it/s, v_num=311, trainLoss=2.830]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([196990, 10, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name     | Type             | Params | Mode \n",
      "------------------------------------------------------\n",
      "0 | model    | Modena           | 1.7 K  | train\n",
      "1 | lossFunc | CrossEntropyLoss | 0      | train\n",
      "------------------------------------------------------\n",
      "1.4 K     Trainable params\n",
      "280       Non-trainable params\n",
      "1.7 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "27        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model got 625904/1969900 right. Accuracy of 31.77338951215798%\n",
      "40.353165135286055% Zeroes.\n",
      "Epoch 9: 100%|██████████| 1/1 [00:00<00:00, 13.47it/s, v_num=312, trainLoss=3.070]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 1/1 [00:00<00:00, 10.22it/s, v_num=312, trainLoss=3.070]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([196990, 10, 20])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[74], line 46\u001b[0m\n\u001b[0;32m     44\u001b[0m         nextModel \u001b[38;5;241m=\u001b[39m OBU(nextModel \u001b[38;5;241m+\u001b[39m mod)\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;66;03m# Test individual model\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m     _, _ , perc, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtestDataIn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtestDataOut\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend([epoch, i, perc])\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Create combined model\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[73], line 81\u001b[0m, in \u001b[0;36mOBU.test\u001b[1;34m(self, dataIn, dataOut, extraLayer)\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m dataOut[i,t] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     79\u001b[0m             \u001b[38;5;66;03m# If the label is zero, increment the count of zeroes to determine if model is just outputting zeroes\u001b[39;00m\n\u001b[0;32m     80\u001b[0m             numZero \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 81\u001b[0m         total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;66;03m# Calculate percent correct and percent zero\u001b[39;00m\n\u001b[0;32m     83\u001b[0m perc \u001b[38;5;241m=\u001b[39m (countR\u001b[38;5;241m/\u001b[39mtotal) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# loop through split, and create model per vehicle.\n",
    "# Run one epoch through each model, and average model and return.\n",
    "# Continue loop for desired epochs\n",
    "\n",
    "# With 2 epochs, 10 sub, and 0:3 models ending acc. of 0.1377%\n",
    "# With 2 epochs, 10 sub, and 0:10 models ending acc. of \n",
    "models = []\n",
    "dataSets = []\n",
    "results = []\n",
    "subEpochs = 10\n",
    "epochs = 2\n",
    "gpu = False\n",
    "# Create starting models\n",
    "mainModel = OBU(9, epochs= subEpochs, gpu = gpu)\n",
    "nextModel = OBU(9, epochs= subEpochs, gpu = gpu)\n",
    "# Divide dataset of recieving vehicles among OBUs\n",
    "for vehicle in splits[0:10]:\n",
    "    # Add new OBU for each model\n",
    "    models.append(OBU(9, epochs = subEpochs, gpu=gpu))\n",
    "    # Create Slice of dataset\n",
    "    vehicle = data.DataLoader(data.TensorDataset(vehicle[:,:,1:10].float(), vehicle[:,:,11].long()), batch_size=batchSize, shuffle=False, num_workers=16, persistent_workers = True, drop_last= True)\n",
    "    # Add sub - dataset to dataset\n",
    "    dataSets.append(vehicle)\n",
    "\n",
    "# Train individual models and combine\n",
    "for epoch in range(epochs):\n",
    "    i = 0\n",
    "    # Baseline model to add everything to. !!Do I want this or should it be a completely new model?!! Got 0% on combination before, testing with new model for next model.\n",
    "    nextModel = OBU(9, epochs= subEpochs, gpu = gpu)\n",
    "    # Train models\n",
    "    for mod in models:\n",
    "        # Make multithreaded?\n",
    "        # set model to main model, and train that\n",
    "        mod = OBU(mainModel)\n",
    "        # Reset the trainer (should be unneeded now) to allow for further training\n",
    "        # mod.resetTrainer()\n",
    "        # Actually train\n",
    "        mod.fit(dataSets[i])\n",
    "        i += 1\n",
    "        # combine models\n",
    "        if not nextModel:\n",
    "            nextModel = OBU(mod)\n",
    "        else:\n",
    "            nextModel = OBU(nextModel + mod)\n",
    "        # Test individual model\n",
    "        _, _ , perc, _ = mod.test(testDataIn, testDataOut)\n",
    "        results.append([epoch, i, perc])\n",
    "    # Create combined model\n",
    "    print(nextModel.getHidden())\n",
    "    mainModel = OBU(nextModel / i)\n",
    "# Test combined model at end\n",
    "mainModel.test(testDataIn, testDataOut)\n",
    "print(results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kettering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
