{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3842ba7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Imports:\n",
    "from ncps.wirings import AutoNCP\n",
    "from ncps.torch import CfC\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from numpy import genfromtxt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import matplotlib as plt\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "pl.seed_everything(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae4e3f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/7798 [54:48<?, ?it/s]\n",
      "(3160001, 12)\n",
      "torch.Size([623867, 10, 12])\n"
     ]
    }
   ],
   "source": [
    "#Dataset Formatting\n",
    "#Generate Time sequences that are 10 timepoints (Messages) with 7 features per message.\n",
    "#Organized by car.\n",
    "\n",
    "#Current Simulation File\n",
    "dataFile = 'Data/CfCMultiExtension/RandomPos_0709.csv' # DoS_0709\n",
    "\n",
    "dataSet = genfromtxt(dataFile, delimiter=',')\n",
    "batchSize = 64\n",
    "# Ceate dataloader and fill with (BSM, attk#). Expanding to add 0th dimension for batches.\n",
    "# Batch size should be 64 for the low density simulations and 128 for high density simulations.\n",
    "# No shuffle to keep batches on same vehicle.\n",
    "# Num_workers is set to = num CPU cores\n",
    "dataSet[0:-1,:] = dataSet[1:,:] # Get rid of the first null value of the dataset\n",
    "print(dataSet.shape)\n",
    "# count subsets per vehicle\n",
    "unq, counts = np.unique(dataSet[:, 2], return_counts = True)\n",
    "sender = 0\n",
    "lastSenderCount = 0\n",
    "newData = []\n",
    "# Organize dataset into sets of 10 messages by sender\n",
    "while sender < counts.shape[0]:\n",
    "    # Loop through sender\n",
    "    index = 0\n",
    "    while index < counts[sender] - 10:\n",
    "        # Loop through messages from sender\n",
    "        newData.append(dataSet[lastSenderCount+index:lastSenderCount +index+10])\n",
    "        index += 5\n",
    "    sender += 1\n",
    "    lastSenderCount += counts[sender-1]\n",
    "dataSet = torch.tensor(newData)\n",
    "leng = dataSet.shape[0]\n",
    "trainPerc = 80\n",
    "# Create new arrays per vehicle for federated learning\n",
    "splits = np.split(dataSet, np.cumsum(counts)[:-1])\n",
    "# Create seperate datasets for testing and training, using Train Percentage as metric for split\n",
    "trainDataIn = torch.Tensor(dataSet[:int(leng*(trainPerc/100)),:,3:10]).float() # 1\n",
    "trainDataOut = torch.Tensor(np.int_(dataSet[:int(leng*(trainPerc/100)),:,11])).long()\n",
    "testDataIn = torch.Tensor(dataSet[int(leng*(trainPerc/100)):,:,3:10]).float() # 1\n",
    "testDataOut = torch.Tensor(np.int_(dataSet[int(leng*(trainPerc/100)):,:,11])).long()\n",
    "newsetIn = []\n",
    "newsetOut = []\n",
    "testsetIn = []\n",
    "testsetOut = []\n",
    "# Create dataset of 1/100th of the entries for quicker testing during development\n",
    "for index in range(0,int(leng * (trainPerc/100))):\n",
    "    if not (int(index/10) % 10):\n",
    "        newsetIn.append(dataSet[index,:,3:10]) # 1\n",
    "        newsetOut.append((dataSet[index,:,11]))\n",
    "testingIn = torch.Tensor(np.array(newsetIn)).float()\n",
    "testingOut = torch.Tensor(np.array(newsetOut)).long()\n",
    "for idx in range(int((leng) * (trainPerc/100)), leng):\n",
    "    if not (int(idx/10) % 10):\n",
    "        testsetIn.append(dataSet[idx,:,3:10])\n",
    "        testsetOut.append((dataSet[idx,:,11]))\n",
    "testingIn = torch.Tensor(np.array(newsetIn)).float()\n",
    "testingOut = torch.Tensor(np.array(newsetOut)).long()\n",
    "inTest = torch.Tensor(np.array(testsetIn)).float()\n",
    "outTest = torch.Tensor(np.array(testsetOut)).long()\n",
    "# Create Dataloaders for all the datasets\n",
    "dataLoaderTrain = data.DataLoader(data.TensorDataset(trainDataIn, trainDataOut), batch_size=batchSize, shuffle=False, num_workers=10, persistent_workers = True, drop_last= True)\n",
    "dataLoaderTest = data.DataLoader(data.TensorDataset(testDataIn, testDataOut), batch_size=batchSize, shuffle=False, num_workers=10, persistent_workers = True, drop_last= True)\n",
    "testingDataLoader = data.DataLoader(data.TensorDataset(testingIn, testingOut), batch_size=batchSize, shuffle = False, num_workers=10, persistent_workers = True, drop_last= True)\n",
    "testingTestData = data.DataLoader(data.TensorDataset(testingIn, testingOut), batch_size=batchSize, shuffle = False, num_workers=10, persistent_workers = True, drop_last= True)\n",
    "print(dataSet.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b7f6e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutLogger():\n",
    "    def __init__(self, path):\n",
    "        #Helpers\n",
    "        self.path = path\n",
    "        self.epochTimes = []\n",
    "        self.times = []\n",
    "\n",
    "        #Outs\n",
    "        self.avgLossVEpoch = []\n",
    "        self.avgF1VEpoch = []\n",
    "        self.avgRecallVEpoch = []\n",
    "        self.avgPrecisionVEpoch = []\n",
    "        self.avgAccuracyVEpoch = []\n",
    "        self.lossVPercEvil = None\n",
    "        self.F1VPercEvil = None\n",
    "        self.RecallVPercEvil = None\n",
    "        self.PrecisionVPercEvil = None\n",
    "        self.AccuracyVPercEvil = None\n",
    "        self.AvgVehicleTime = None\n",
    "        self.MaxVehicleTime = None\n",
    "        self.TotTime = None\n",
    "\n",
    "    def startVehicleTimer(self):\n",
    "        self.startTime = time.time()\n",
    "    \n",
    "    def endVehicleTimer(self):\n",
    "        self.times.append(time.time()-self.startTime)\n",
    "\n",
    "    def startEpochTimer(self):\n",
    "        self.startEpochTime = time.time()\n",
    "    \n",
    "    def endEpochTimer(self):\n",
    "        self.epochTimes.append(time.time()-self.startEpochTime)\n",
    "\n",
    "    def updateLogs(self, vehicles, epoch):\n",
    "        currLoss = 0\n",
    "        currF1 = 0\n",
    "        currRecall = 0\n",
    "        currPrecision = 0\n",
    "        currAccuracy = 0\n",
    "        count = 0\n",
    "        for vehicle in vehicles:\n",
    "            currLoss += vehicle.curr_loss\n",
    "            f1, recall, precision, accuracy = vehicle.test(testDataIn, testDataOut, True)\n",
    "            currF1 += f1\n",
    "            currRecall += recall\n",
    "            currPrecision += precision\n",
    "            currAccuracy += accuracy\n",
    "            count += 1\n",
    "        self.avgLossVEpoch.append([epoch, currLoss/count])\n",
    "        self.avgF1VEpoch.append([epoch, currF1/count])\n",
    "        self.avgRecallVEpoch.append([epoch, currRecall/count])\n",
    "        self.avgPrecisionVEpoch.append([epoch, currPrecision/count])\n",
    "        self.avgAccuracyVEpoch.append([epoch, currAccuracy/count])\n",
    "            \n",
    "\n",
    "    def finalLogs(self, percEvil):\n",
    "        self.lossVPercEvil = [percEvil, self.avgLossVEpoch[-1][1]]\n",
    "        self.F1VPercEvil = [percEvil, self.avgF1VEpoch[-1][1]]\n",
    "        self.RecallVPercEvil = [percEvil, self.avgRecallVEpoch[-1][1]]\n",
    "        self.PrecisionVPercEvil = [percEvil, self.avgPrecisionVEpoch[-1][1]]\n",
    "        self.AccuracyVPercEvil = [percEvil, self.avgAccuracyVEpoch[-1][1]]\n",
    "        self.AvgVehicleTime = np.sum(self.times)/len(self.times)\n",
    "        self.MaxVehicleTime = np.max(self.times)\n",
    "        self.TotTime = np.sum(self.epochTimes)/len(self.epochTimes)\n",
    "\n",
    "    def log(self):\n",
    "        path = f\"out/{self.path}\"\n",
    "        if not os.path.exists(f\"out/{self.path}\"):\n",
    "            os.makedirs(f\"out/{self.path}\")\n",
    "        with open(f'{path}avgLossVEpoch.csv', 'w', newline='') as filename:\n",
    "            writer = csv.writer(filename)\n",
    "            writer.writerow(['epoch', 'avg Loss'])\n",
    "            writer.writerows(self.avgLossVEpoch)\n",
    "        with open(f'{path}avgF1VEpoch.csv', 'w', newline='') as filename:\n",
    "            writer = csv.writer(filename)\n",
    "            writer.writerow(['epoch', 'avg F1'])\n",
    "            writer.writerows(self.avgF1VEpoch)\n",
    "        with open(f'{path}avgRecallVEpoch.csv', 'w', newline='') as filename:\n",
    "            writer = csv.writer(filename)\n",
    "            writer.writerow(['epoch', 'avg Recall'])\n",
    "            writer.writerows(self.avgRecallVEpoch)\n",
    "        with open(f'{path}avgPrecisionVEpoch.csv', 'w', newline='') as filename:\n",
    "            writer = csv.writer(filename)\n",
    "            writer.writerow(['epoch', 'avg Precision'])\n",
    "            writer.writerows(self.avgPrecisionVEpoch)\n",
    "        with open(f'{path}avgAccuracyVEpoch.csv', 'w', newline='') as filename:\n",
    "            writer = csv.writer(filename)\n",
    "            writer.writerow(['epoch', 'avg Accuracy'])\n",
    "            writer.writerows(self.avgAccuracyVEpoch)\n",
    "        others = {'Loss V PercEvil':self.lossVPercEvil, 'F1 V PercEvil':self.F1VPercEvil, 'Recall V PercEvil':self.RecallVPercEvil, 'Precision V PercEvil':self.PrecisionVPercEvil, \n",
    "                  'Accuracy V PercEvil':self.AccuracyVPercEvil, 'Max Per-Vehicle Time':self.MaxVehicleTime, 'Avg Per-Vehicle Time':self.AvgVehicleTime, 'Total Time Per Epoch':self.TotTime}\n",
    "        with open(f'{path}avgAccuracyVEpoch.csv', 'w', newline='') as filename:\n",
    "            writer = csv.writer(filename)\n",
    "            writer.writerow(['epoch', 'avg Accuracy'])\n",
    "            writer.writerows(self.avgAccuracyVEpoch)\n",
    "        with open(f'{path}ExtraData.json', 'w') as filename:\n",
    "            json.dump(others, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e6ee4b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "960.0\n"
     ]
    }
   ],
   "source": [
    "tom = []\n",
    "for line in dataSet:\n",
    "    tom.append(line.nbytes)\n",
    "print(np.average(tom))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f4ff8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Learner\n",
    "class CfCLearner(pl.LightningModule):\n",
    "    def __init__(self, model, lr):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.lossFunc = nn.CrossEntropyLoss()\n",
    "        self.loss = None\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Get in and out from batch\n",
    "        inputs, target = batch\n",
    "        # Put input through model\n",
    "        output, _ = self.model.forward(inputs)\n",
    "        # Reorganize inputs for use with loss function\n",
    "        output = output.permute(0, 2, 1)\n",
    "        # Calculate Loss using Cross Entropy Loss \n",
    "        loss = self.lossFunc(output, target)\n",
    "        self.log(\"trainLoss\", loss, prog_bar=True)\n",
    "        self.loss = loss\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # Get in and out from batch\n",
    "        inputs, target = batch\n",
    "        # Put input through model\n",
    "        output, _ = self.model.forward(inputs)\n",
    "        # Reorganize inputs for use with loss function\n",
    "        output = output.permute(0, 2, 1)\n",
    "        print(f\"output: {output.shape}\")\n",
    "        print(f\"target: {target.shape}\")\n",
    "        # Calculate Loss using Cross Entropy Loss \n",
    "        loss = self.lossFunc(output, target)\n",
    "        self.log(\"valLoss\", loss, prog_bar=True)\n",
    "        self.loss = loss\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self.validation_step(batch, batch_idx)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        # Using AdamW optomizer based on info from paper\n",
    "        # self.lr\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr = 0.001)\n",
    "        return ([optimizer], [torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.6)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "28b52804",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Modena(nn.Module): \n",
    "    # CfC with feed-forward layer to classify at end.\n",
    "    def __init__(self, inputSize, unitNum, motorNum, outputDim, batchFirst = True):\n",
    "        super().__init__()\n",
    "        # Create NCP wiring for CfC\n",
    "        wiring = AutoNCP(unitNum, motorNum)\n",
    "        # Create CfC model with inputs and wiring\n",
    "        self.cfc = CfC(inputSize, wiring, batch_first=batchFirst)\n",
    "        # Create feed-forward layer\n",
    "        self.fF = nn.Linear(motorNum, outputDim)\n",
    "    \n",
    "    def forward(self, batch, hidden = None):\n",
    "        batch, hidden = self.cfc(batch, hidden) # Pass inputs through CfC\n",
    "        out = nn.functional.relu(self.fF(batch)) # pass through FeedForward Layer, then make 0 minimum\n",
    "        return out, hidden # Return the guess and the hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "02b7a02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNLSTM(nn.Module):\n",
    "    def __init__(self, inputSize, outputDim, lSTMinFeat = 10, batchFirst = True):\n",
    "        super(CNNLSTM, self).__init__()\n",
    "        self.inputSize = inputSize\n",
    "        self.outputDim = outputDim\n",
    "        self.lSTMinFeat = lSTMinFeat\n",
    "        self. batchFirst = batchFirst\n",
    "        # CNN layer \n",
    "        self.cnn = nn.Conv1d(10, lSTMinFeat, kernel_size=inputSize, padding=3) # 10 is size of in sequences, then size of outputs, then the number of parameters per sequence\n",
    "        # LSTM Layers in of LSTM should be same as out of CNN - 20 x \n",
    "        self.lstm = nn.LSTM(inputSize, 256, 4, batch_first=batchFirst) # \n",
    "        # Linear Layer (SVM)\n",
    "        self.linear = nn.Linear(256, outputDim)\n",
    "\n",
    "    def forward(self, batch, hidden = None):\n",
    "        batch = self.cnn(batch)\n",
    "        print(batch.shape)\n",
    "        batch, hidden = self.lstm(batch)\n",
    "        print(batch.shape)\n",
    "        batch = self.linear(batch)\n",
    "        print(batch.shape)\n",
    "        out = nn.functional.relu(batch)\n",
    "        print(out.shape)\n",
    "        return out, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ecad7b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OBU module class to organize\n",
    "class OBU():\n",
    "    def __init__(self, inputSize = 9, units = 20, motors = 8, outputs = 20, epochs = 0, lr = 0.001, gpu = False):\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.gpu = gpu\n",
    "        self.model = Modena(inputSize, units, motors, outputs)\n",
    "        self.learner = CfCLearner(self.model, lr) # tune units, lr\n",
    "        self.trainer = pl.Trainer(\n",
    "            logger = CSVLogger('log/Non-Fed'), # Set ouput destination of logs, logging accuracy every 50 steps\n",
    "            max_epochs = epochs, # Number of epochs to train for\n",
    "            gradient_clip_val = 1, # This is said to stabilize training, but we should test if that is true\n",
    "            accelerator = \"gpu\" if gpu else \"cpu\" # Using the GPU to run training or not\n",
    "            )\n",
    "        self.curr_loss = None\n",
    "    \n",
    "    def fit(self, dataLoader):\n",
    "        # calling built in fit function\n",
    "        self.trainer.fit(self.learner, dataLoader)\n",
    "        return self.learner.loss\n",
    "    \n",
    "    def step(self, epochs, dataLoader):\n",
    "        self.trainer.fit_loop.max_epochs = self.trainer.current_epoch + epochs\n",
    "        self.curr_loss = self.fit(dataLoader).item()\n",
    "    \n",
    "    def train(self, epochs, dataLoader, log):\n",
    "        epoch = 0\n",
    "        while epoch < epochs:\n",
    "            log.startEpochTimer()\n",
    "            log.startVehicleTimer()\n",
    "            self.step(1, dataLoader)\n",
    "            log.endEpochTimer()\n",
    "            log.endVehicleTimer()\n",
    "            log.updateLogs([self], epoch)\n",
    "            epoch += 1\n",
    "        log.finalLogs(0)\n",
    "        log.log()\n",
    "\n",
    "    \n",
    "    # Function to run model through a testing dataset and calculate accuracy. Can be expanded to give more metrics and more useful metrics.\n",
    "    def test(self, dataIn, dataOut, mathy = False):\n",
    "        # Put input data through model and determine classification\n",
    "        with torch.no_grad():\n",
    "            outs = np.asarray(self.model(dataIn)[0])\n",
    "        outs = torch.from_numpy(outs)\n",
    "        # Get the label with the maximum confidence for determining classification\n",
    "        print(outs.shape)\n",
    "        _, res = torch.max(outs, 2)\n",
    "        Pt = Pf = Nt = Nf = 0\n",
    "        countR = 0\n",
    "        numZero = 0\n",
    "        tot = outs.shape[0]\n",
    "        total = 0\n",
    "        for i in range(0, tot):\n",
    "            # Loop through sequences of 10 each\n",
    "            for t in range(0, res[i].shape[0]):\n",
    "                # Loop through the sub-sequences\n",
    "                if res[i,t] == dataOut[i,t]:\n",
    "                    if res[i,t] == 0:\n",
    "                        Nt += 1\n",
    "                        numZero += 1\n",
    "                    else:\n",
    "                        Pt += 1\n",
    "                    # Check if label is correct, and add to count right accordingly\n",
    "                    countR += 1\n",
    "                else:\n",
    "                    if dataOut[i,t] == 0:\n",
    "                        Pf += 1\n",
    "                        numZero += 1\n",
    "                    else:\n",
    "                        Nf += 1\n",
    "                total += 1\n",
    "        # Calculate percent correct and percent zero\n",
    "        if mathy:\n",
    "            if Pt != 0:\n",
    "                accuracy = (Pt+Nt)/(Pt+Pf+Nf+Nt)\n",
    "                precision = (Pt)/(Pt+Pf)\n",
    "                recall = (Pt)/(Pt+Nf)\n",
    "                f1 = (2*precision*recall)/(precision+recall)\n",
    "                print(precision)\n",
    "                print(recall)\n",
    "                print(\"Model got \" + str(countR) + \"/\" + str(total) + \" right.\")\n",
    "                print(f\"Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1 Score: {f1}\")\n",
    "                print(f\"{numZero}, {numZero/total * 100}% Zeroes, {total-numZero} Non Zero entries.\")\n",
    "                return f1, recall, precision, accuracy\n",
    "            else:\n",
    "                print(\"Model could not complete tests.\")\n",
    "                return 0, 0, 0, 0\n",
    "        else:\n",
    "            if Pt != 0:\n",
    "                accuracy = (Pt+Nt)/(Pt+Pf+Nf+Nt)\n",
    "                precision = (Pt)/(Pt+Pf)\n",
    "                recall = (Pt)/(Pt+Nf)\n",
    "                f1 = (2*precision*recall)/(precision+recall)\n",
    "                print(precision)\n",
    "                print(recall)\n",
    "                print(\"Model got \" + str(countR) + \"/\" + str(total) + \" right.\")\n",
    "                print(f\"Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1 Score: {f1}\")\n",
    "                print(f\"{numZero}, {numZero/total * 100}% Zeroes, {total-numZero} Non Zero entries.\")\n",
    "                return f\"Model got {countR}/{total} right. Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1 Score: {f1}\"\n",
    "            else:\n",
    "                print(\"Model could not complete tests.\")\n",
    "                return f\"Model could not complete tests, found 0 of misbehaviour.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ba4737bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OBU module class to organize\n",
    "class lstmOBU():\n",
    "    def __init__(self, inputSize = 10, units = 20, motors = 8, outputs = 20, epochs = 0, lr = 0.001, gpu = False):\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.gpu = gpu\n",
    "        self.model = CNNLSTM(inputSize, outputs)\n",
    "        self.learner = CfCLearner(self.model, lr) # tune units, lr\n",
    "        self.trainer = pl.Trainer(\n",
    "            logger = CSVLogger('log/Non-Fed'), # Set ouput destination of logs, logging accuracy every 50 steps\n",
    "            max_epochs = epochs, # Number of epochs to train for\n",
    "            gradient_clip_val = 1, # This is said to stabilize training, but we should test if that is true\n",
    "            accelerator = \"gpu\" if gpu else \"cpu\" # Using the GPU to run training or not\n",
    "            )\n",
    "        self.curr_loss = None\n",
    "    \n",
    "    def fit(self, dataLoader):\n",
    "        # calling built in fit function\n",
    "        self.trainer.fit(self.learner, dataLoader)\n",
    "        return self.learner.loss\n",
    "    \n",
    "    def step(self, epochs, dataLoader):\n",
    "        self.trainer.fit_loop.max_epochs = self.trainer.current_epoch + epochs\n",
    "        self.curr_loss = self.fit(dataLoader).item()\n",
    "    \n",
    "    def train(self, epochs, dataLoader, log):\n",
    "        epoch = 0\n",
    "        while epoch < epochs:\n",
    "            log.startEpochTimer()\n",
    "            log.startVehicleTimer()\n",
    "            self.step(1, dataLoader)\n",
    "            log.endEpochTimer()\n",
    "            log.endVehicleTimer()\n",
    "            log.updateLogs([self], epoch)\n",
    "            epoch += 1\n",
    "        log.finalLogs(0)\n",
    "        log.log()\n",
    "\n",
    "    \n",
    "    # Function to run model through a testing dataset and calculate accuracy. Can be expanded to give more metrics and more useful metrics.\n",
    "    def test(self, dataIn, dataOut, mathy = False):\n",
    "        # Put input data through model and determine classification\n",
    "        with torch.no_grad():\n",
    "            outs = np.asarray(self.model(dataIn)[0])\n",
    "        outs = torch.from_numpy(outs)\n",
    "        # Get the label with the maximum confidence for determining classification\n",
    "        print(outs.shape)\n",
    "        _, res = torch.max(outs, 2)\n",
    "        Pt = Pf = Nt = Nf = 0\n",
    "        countR = 0\n",
    "        numZero = 0\n",
    "        tot = outs.shape[0]\n",
    "        total = 0\n",
    "        for i in range(0, tot):\n",
    "            # Loop through sequences of 10 each\n",
    "            for t in range(0, res[i].shape[0]):\n",
    "                # Loop through the sub-sequences\n",
    "                if res[i,t] == dataOut[i,t]:\n",
    "                    if res[i,t] == 0:\n",
    "                        Nt += 1\n",
    "                        numZero += 1\n",
    "                    else:\n",
    "                        Pt += 1\n",
    "                    # Check if label is correct, and add to count right accordingly\n",
    "                    countR += 1\n",
    "                else:\n",
    "                    if dataOut[i,t] == 0:\n",
    "                        Pf += 1\n",
    "                        numZero += 1\n",
    "                    else:\n",
    "                        Nf += 1\n",
    "                total += 1\n",
    "        # Calculate percent correct and percent zero\n",
    "        if mathy:\n",
    "            if Pt != 0:\n",
    "                accuracy = (Pt+Nt)/(Pt+Pf+Nf+Nt)\n",
    "                precision = (Pt)/(Pt+Pf)\n",
    "                recall = (Pt)/(Pt+Nf)\n",
    "                f1 = (2*precision*recall)/(precision+recall)\n",
    "                print(precision)\n",
    "                print(recall)\n",
    "                print(\"Model got \" + str(countR) + \"/\" + str(total) + \" right.\")\n",
    "                print(f\"Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1 Score: {f1}\")\n",
    "                print(f\"{numZero}, {numZero/total * 100}% Zeroes, {total-numZero} Non Zero entries.\")\n",
    "                return f1, recall, precision, accuracy\n",
    "            else:\n",
    "                print(\"Model could not complete tests.\")\n",
    "                return 0, 0, 0, 0\n",
    "        else:\n",
    "            if Pt != 0:\n",
    "                accuracy = (Pt+Nt)/(Pt+Pf+Nf+Nt)\n",
    "                precision = (Pt)/(Pt+Pf)\n",
    "                recall = (Pt)/(Pt+Nf)\n",
    "                f1 = (2*precision*recall)/(precision+recall)\n",
    "                print(precision)\n",
    "                print(recall)\n",
    "                print(\"Model got \" + str(countR) + \"/\" + str(total) + \" right.\")\n",
    "                print(f\"Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1 Score: {f1}\")\n",
    "                print(f\"{numZero}, {numZero/total * 100}% Zeroes, {total-numZero} Non Zero entries.\")\n",
    "                return f\"Model got {countR}/{total} right. Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1 Score: {f1}\"\n",
    "            else:\n",
    "                print(\"Model could not complete tests.\")\n",
    "                return f\"Model could not complete tests, found 0 of misbehaviour.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f62e03e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/will/.conda/envs/Kettering/lib/python3.13/site-packages/pytorch_lightning/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "lr = 0.01\n",
    "testOBU = lstmOBU(\n",
    "    inputSize = 7, # 9  # Number of features per BSM\n",
    "    units = 20, # Number of hidden cells\n",
    "    motors = 8, # Number of motor neurons\n",
    "    outputs = 20, # Number of possible labels\n",
    "    lr = lr, # 0.001\n",
    "    gpu = False\n",
    ")\n",
    "path = f\"Normal/cnn-{epochs}-{lr}/\"\n",
    "\n",
    "log = OutLogger(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be79676f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cfc.rnn_cell.layer_0.sparsity_mask\n",
      "cfc.rnn_cell.layer_0.ff1.weight\n",
      "cfc.rnn_cell.layer_0.ff1.bias\n",
      "cfc.rnn_cell.layer_0.ff2.weight\n",
      "cfc.rnn_cell.layer_0.ff2.bias\n",
      "cfc.rnn_cell.layer_0.time_a.weight\n",
      "cfc.rnn_cell.layer_0.time_a.bias\n",
      "cfc.rnn_cell.layer_0.time_b.weight\n",
      "cfc.rnn_cell.layer_0.time_b.bias\n",
      "cfc.rnn_cell.layer_1.sparsity_mask\n",
      "cfc.rnn_cell.layer_1.ff1.weight\n",
      "cfc.rnn_cell.layer_1.ff1.bias\n",
      "cfc.rnn_cell.layer_1.ff2.weight\n",
      "cfc.rnn_cell.layer_1.ff2.bias\n",
      "cfc.rnn_cell.layer_1.time_a.weight\n",
      "cfc.rnn_cell.layer_1.time_a.bias\n",
      "cfc.rnn_cell.layer_1.time_b.weight\n",
      "cfc.rnn_cell.layer_1.time_b.bias\n",
      "cfc.rnn_cell.layer_2.sparsity_mask\n",
      "cfc.rnn_cell.layer_2.ff1.weight\n",
      "cfc.rnn_cell.layer_2.ff1.bias\n",
      "cfc.rnn_cell.layer_2.ff2.weight\n",
      "cfc.rnn_cell.layer_2.ff2.bias\n",
      "cfc.rnn_cell.layer_2.time_a.weight\n",
      "cfc.rnn_cell.layer_2.time_a.bias\n",
      "cfc.rnn_cell.layer_2.time_b.weight\n",
      "cfc.rnn_cell.layer_2.time_b.bias\n",
      "fF.weight\n",
      "fF.bias\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpt = torch.load('log/Non-Fed/lightning_logs/version_24/checkpoints/epoch=9-step=128955.ckpt') # Load checkpoint //Version 3627 is with sender and reciever names baked in, and epoch 85\n",
    "newDict = {}\n",
    "for key in checkpt['state_dict'].keys(): # Loop through the checkpoint and get the keys, since the checkpoint stores the keys under the incorrect labels.\n",
    "    val = checkpt['state_dict'][key]\n",
    "    key = key[6:]\n",
    "    newDict[key] = val\n",
    "    print(key)\n",
    "checkpt['state_dict'] = newDict\n",
    "testOBU.model.load_state_dict(checkpt['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "56f72121",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'lstmOBU' object has no attribute 'testStep'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[107]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtestOBU\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtestStep\u001b[49m(dataLoaderTrain)\n",
      "\u001b[31mAttributeError\u001b[39m: 'lstmOBU' object has no attribute 'testStep'"
     ]
    }
   ],
   "source": [
    "testOBU.testStep(dataLoaderTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b50140ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Training:\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "input.size(-1) must be equal to input_size. Expected 20, got 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBefore Training:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mtestOBU\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtestDataIn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtestDataOut\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 44\u001b[39m, in \u001b[36mlstmOBU.test\u001b[39m\u001b[34m(self, dataIn, dataOut, mathy)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtest\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataIn, dataOut, mathy = \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m     42\u001b[39m     \u001b[38;5;66;03m# Put input data through model and determine classification\u001b[39;00m\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m         outs = np.asarray(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataIn\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m])\n\u001b[32m     45\u001b[39m     outs = torch.from_numpy(outs)\n\u001b[32m     46\u001b[39m     \u001b[38;5;66;03m# Get the label with the maximum confidence for determining classification\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/Kettering/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/Kettering/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mCNNLSTM.forward\u001b[39m\u001b[34m(self, batch, hidden)\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, hidden = \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     16\u001b[39m     batch = \u001b[38;5;28mself\u001b[39m.cnn(batch)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     batch = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m     batch = \u001b[38;5;28mself\u001b[39m.linear(batch,)\n\u001b[32m     19\u001b[39m     out = nn.functional.relu(batch)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/Kettering/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/Kettering/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/Kettering/lib/python3.13/site-packages/torch/nn/modules/rnn.py:1101\u001b[39m, in \u001b[36mLSTM.forward\u001b[39m\u001b[34m(self, input, hx)\u001b[39m\n\u001b[32m   1093\u001b[39m     c_zeros = torch.zeros(\n\u001b[32m   1094\u001b[39m         \u001b[38;5;28mself\u001b[39m.num_layers * num_directions,\n\u001b[32m   1095\u001b[39m         max_batch_size,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1098\u001b[39m         device=\u001b[38;5;28minput\u001b[39m.device,\n\u001b[32m   1099\u001b[39m     )\n\u001b[32m   1100\u001b[39m     hx = (h_zeros, c_zeros)\n\u001b[32m-> \u001b[39m\u001b[32m1101\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcheck_forward_args\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1102\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1103\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_batched:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/Kettering/lib/python3.13/site-packages/torch/nn/modules/rnn.py:1002\u001b[39m, in \u001b[36mLSTM.check_forward_args\u001b[39m\u001b[34m(self, input, hidden, batch_sizes)\u001b[39m\n\u001b[32m    996\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcheck_forward_args\u001b[39m(\n\u001b[32m    997\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    998\u001b[39m     \u001b[38;5;28minput\u001b[39m: Tensor,\n\u001b[32m    999\u001b[39m     hidden: \u001b[38;5;28mtuple\u001b[39m[Tensor, Tensor],  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[32m   1000\u001b[39m     batch_sizes: Optional[Tensor],\n\u001b[32m   1001\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1002\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1003\u001b[39m     \u001b[38;5;28mself\u001b[39m.check_hidden_size(\n\u001b[32m   1004\u001b[39m         hidden[\u001b[32m0\u001b[39m],\n\u001b[32m   1005\u001b[39m         \u001b[38;5;28mself\u001b[39m.get_expected_hidden_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[32m   1006\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mExpected hidden[0] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1007\u001b[39m     )\n\u001b[32m   1008\u001b[39m     \u001b[38;5;28mself\u001b[39m.check_hidden_size(\n\u001b[32m   1009\u001b[39m         hidden[\u001b[32m1\u001b[39m],\n\u001b[32m   1010\u001b[39m         \u001b[38;5;28mself\u001b[39m.get_expected_cell_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[32m   1011\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mExpected hidden[1] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1012\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/Kettering/lib/python3.13/site-packages/torch/nn/modules/rnn.py:314\u001b[39m, in \u001b[36mRNNBase.check_input\u001b[39m\u001b[34m(self, input, batch_sizes)\u001b[39m\n\u001b[32m    310\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    311\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33minput must have \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_input_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m dimensions, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m.dim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    312\u001b[39m     )\n\u001b[32m    313\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.input_size != \u001b[38;5;28minput\u001b[39m.size(-\u001b[32m1\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    315\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33minput.size(-1) must be equal to input_size. Expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.input_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m.size(-\u001b[32m1\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    316\u001b[39m     )\n",
      "\u001b[31mRuntimeError\u001b[39m: input.size(-1) must be equal to input_size. Expected 20, got 1"
     ]
    }
   ],
   "source": [
    "print(\"Before Training:\")\n",
    "testOBU.test(testDataIn, testDataOut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "5e231a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name     | Type             | Params | Mode \n",
      "------------------------------------------------------\n",
      "0 | model    | CNNLSTM          | 1.9 M  | train\n",
      "1 | lossFunc | CrossEntropyLoss | 0      | train\n",
      "------------------------------------------------------\n",
      "1.9 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 M     Total params\n",
      "7.425     Total estimated model params size (MB)\n",
      "5         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7798/7798 [06:15<00:00, 20.77it/s, v_num=60, trainLoss=0.0459] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7798/7798 [06:15<00:00, 20.77it/s, v_num=60, trainLoss=0.0459]\n",
      "torch.Size([124774, 10, 20])\n",
      "Model could not complete tests.\n"
     ]
    }
   ],
   "source": [
    "# Training dataset - much larger, but more accurate\n",
    "\n",
    "testOBU.train(epochs, dataLoaderTrain, log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2163c475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2336\n"
     ]
    }
   ],
   "source": [
    "print(sys.getsizeof(testOBU.learner.state_dict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61352918",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/will/.conda/envs/Kettering/lib/python3.13/site-packages/pytorch_lightning/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "\n",
      "  | Name     | Type             | Params | Mode \n",
      "------------------------------------------------------\n",
      "0 | model    | Modena           | 1.6 K  | train\n",
      "1 | lossFunc | CrossEntropyLoss | 0      | train\n",
      "------------------------------------------------------\n",
      "1.3 K     Trainable params\n",
      "264       Non-trainable params\n",
      "1.6 K     Total params\n",
      "0.006     Total estimated model params size (MB)\n",
      "27        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "`Trainer.fit` stopped: `max_epochs=0` reached.\n"
     ]
    }
   ],
   "source": [
    "# Small subset of training dataset (10%) designed for test running the training\n",
    "testOBU.fit(testingDataLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6ba8da65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Training:\n",
      "torch.Size([124774, 10, 20])\n",
      "Model could not complete tests.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Model could not complete tests, found 0 of misbehaviour.'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"After Training:\")\n",
    "testOBU.test(testDataIn, testDataOut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0af50b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([49910, 10, 20])\n",
      "0.9742246122243374\n",
      "0.9941188251001335\n",
      "Model got 494279/499100 right.\n",
      "Accuracy: 0.9903406131035865, Precision: 0.9742246122243374, Recall: 0.9941188251001335, F1 Score: 0.9840711824198191\n",
      "349300, 69.9859747545582% Zeroes, 149800 Non Zero entries.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Model got 494279/499100 right. Accuracy: 0.9903406131035865, Precision: 0.9742246122243374, Recall: 0.9941188251001335, F1 Score: 0.9840711824198191'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testOBU.test(testingIn, testingOut)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kettering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
